{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cold Users (127,57)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd  \n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "import tensorflow as tf\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_following = np.load('user_following_127_57.npy')\n",
    "user_category = np.load('user_category_127.npy')\n",
    "YouTuber_category = np.load('YouTuber_category_0.7_57.npy')\n",
    "active_users = np.load('active_userID_127.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_test_amount = 25\n",
    "yt_test_amount = 29\n",
    "yt_num = user_following.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test(user_following,user_category,YouTuber_category,active_users):\n",
    "    user_test_amount = 25\n",
    "    yt_test_amount = 29\n",
    "    user_category_norm = np.zeros(user_category.shape)\n",
    "    for i in range(len(user_category)):\n",
    "        user_category_norm[i] = user_category[i]/np.max(user_category[i])\n",
    "    \n",
    "    user_idx = [i for i in range(len(user_following))]\n",
    "    random.seed(5)\n",
    "    train_t = []\n",
    "    train_f = []\n",
    "    test_t = []\n",
    "    test_f = []\n",
    "    #test_idx = np.zeros(user_test_amount)\n",
    "    test_idx = sorted(random.sample(user_idx,user_test_amount))\n",
    "    print(test_idx)\n",
    "    #Train \n",
    "    train_t = []\n",
    "    train_f = []\n",
    "    #Test\n",
    "    test_t = []\n",
    "    test_f = []\n",
    "    test_pos = -1 \n",
    "    for i in range(len(user_following)):\n",
    "        if i in test_idx: #如果i = 那九個>1的\n",
    "            temp_t = [j for j,v in enumerate(user_following[i]) if v==1]\n",
    "            temp_f = [j for j,v in enumerate(user_following[i]) if v==0]\n",
    "            t_for_test = random.sample(temp_t,math.ceil(0.5*len(temp_t)))\n",
    "            f_for_test  = random.sample(temp_f,yt_test_amount-len(t_for_test))\n",
    "            test_t.append(t_for_test)\n",
    "            test_f.append(f_for_test)\n",
    "            #other for training\n",
    "            t_for_train = [item for item in temp_t if not item in t_for_test]\n",
    "            f_for_train = [item for item in temp_f if not item in f_for_test]\n",
    "            train_t.append(t_for_train)\n",
    "            train_f.append(f_for_train)\n",
    "        else:\n",
    "            t_for_train = [j for j,v in enumerate(user_following[i]) if v==1]\n",
    "            f_for_train = [j for j,v in enumerate(user_following[i]) if v==0]\n",
    "            train_t.append(t_for_train)\n",
    "            train_f.append(f_for_train)\n",
    "    \n",
    "    print('The length of train_t:',len(train_t))\n",
    "    print('The length of train_f:',len(train_f))\n",
    "    print('The length of test_t:',len(test_t))\n",
    "    print('The length of test_f:',len(test_f))\n",
    "    return train_t,train_f,test_t,test_f,user_category_norm,test_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 6, 14, 20, 31, 32, 45, 47, 48, 59, 60, 67, 69, 79, 83, 88, 94, 99, 101, 107, 111, 117, 119, 120, 124]\n",
      "The length of train_t: 127\n",
      "The length of train_f: 127\n",
      "The length of test_t: 25\n",
      "The length of test_f: 25\n"
     ]
    }
   ],
   "source": [
    "train_t,train_f,test_t,test_f,user_category_norm,test_idx = get_train_test(user_following,user_category,YouTuber_category,active_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training 4.409448818897638\n",
      "testing 3.0\n",
      "比例: 0.10344827586206896\n"
     ]
    }
   ],
   "source": [
    "#average num of following for training user\n",
    "total_train = 0\n",
    "for t in train_t:\n",
    "    total_train += len(t)\n",
    "avg = total_train/len(user_following)\n",
    "print('training',avg)\n",
    "#average num of following for testing user\n",
    "total_test = 0\n",
    "for t in test_t:\n",
    "    total_test += len(t)\n",
    "avg = total_test/user_test_amount\n",
    "print('testing',avg)\n",
    "print('比例:',avg/yt_test_amount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_3374 = np.load('all_2939D_y57.npy')\n",
    "all_auxilary = [i for i in range(yt_num)]\n",
    "n = len(user_following)\n",
    "m = yt_num  \n",
    "k = 64\n",
    "l = all_3374.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(save_name): \n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    "    loss_acc_list = []\n",
    "    t0=time.time()\n",
    "    \n",
    "    train_yes_id=[] \n",
    "    for q in range(7):\n",
    "        print('Iteraction:',q)\n",
    "        train_auc=0\n",
    "        total_loss=0\n",
    "        xuij_auc=0\n",
    "        length = 0\n",
    "        for z in range(n):\n",
    "            \"\"\"\n",
    "            yes 用來存放選擇到的YouTuber feature (for auxilary)\n",
    "            yesr 用來存放user對該YouTuber的喜好程度(user_category 跟 YouTuber_category的相似性)\n",
    "            r_3 用來存放user 對該YouTuber種類的偏好(取max)\n",
    "            \"\"\"\n",
    "            yes=[]\n",
    "            yesr=[]\n",
    "        \n",
    "            \"\"\"\n",
    "            only choose positive \n",
    "            \"\"\"\n",
    "            #sample=random.sample(train_t[z],len(train_t[z])) #選全部的Positive\n",
    "            \"\"\"\n",
    "            choose all YouTuber \n",
    "            \"\"\"\n",
    "            sample= all_auxilary #選全部的Positive\n",
    "        \n",
    "            #sample=random.sample(train_t[z]+train_f[z],len(train_t[z])+len(train_f[z]))\n",
    "        \n",
    "            #user degree of category favor \n",
    "            r_3=np.zeros(len(sample)) \n",
    "         \n",
    "            for b in range(len(sample)):\n",
    "                yes.append(all_3374[sample[b]])\n",
    "                yesr.append(YouTuber_category[sample[b]]*user_category_norm[z])\n",
    "        \n",
    "            for b in range(len(yesr)):\n",
    "                r_3[b]=max(yesr[b])\n",
    "            #print('r_3:',r_3)\n",
    "        \n",
    "            yes=np.array(yes)\n",
    "        \n",
    "            #取positive \n",
    "            train_t_sample = random.sample(train_t[z],len(train_t[z]))\n",
    "            #print('number of positive feedback', len(train_t_sample))\n",
    "        \n",
    "            #train_f_sample = random.sample(train_f[z],20)\n",
    "            for ta in train_t_sample:\n",
    "                pos = sample.index(ta)\n",
    "                \n",
    "                image_1=np.expand_dims(all_3374[ta],0) #(1,2048)\n",
    "                if len(train_f[z]) > 10:\n",
    "                    train_f_sample = random.sample(train_f[z],10)\n",
    "                else:\n",
    "                    train_f_sample = random.sample(train_f[z],len(train_f[z]))\n",
    "                \n",
    "                for b in train_f_sample:\n",
    "                    image_2=np.expand_dims(all_3374[b],0) #(1,2048)\n",
    "                    _last_be_relu,_norm_par,_a_list,r3,_auc, _loss,_=sess.run([last_be_relu,norm_par,a_list_smooth,a_list_soft,auc,loss,train_op], feed_dict={user: [z],\n",
    "                                        i: [ta], j: [b], xf: yes , l_id:sample, l_id_len:[len(sample)],positive_id:train_t[z],positive_len:[len(train_t[z])],r:r_3,\n",
    "                                        image_i:image_1,image_j:image_2})\n",
    "                    #print(_a_list)\n",
    "                    #print(r3)\n",
    "                    train_auc+=_auc\n",
    "                    total_loss+=_loss\n",
    "                    length += 1\n",
    "        #print('a_list:',_a_list)\n",
    "        #print('a_list_soft:',r3)\n",
    "        print(\"total_loss:-----------------\", total_loss/length)\n",
    "        print(\"train_auc:-------------------\", train_auc/length)\n",
    "        loss_acc_list.append([total_loss/length,train_auc/length,time.time()-t0])\n",
    "        print('time:',time.time()-t0,' sec')\n",
    "    print('Total cost ',time.time()-t0,' sec')   \n",
    "    U, Y, A, E, Au, Ay, Aa, Av,B =sess.run([user_latent, item_latent, aux_item, embedding, Wu, Wy, Wa, Wv,Beta])\n",
    "    np.savez('./Result/Cold_user/'+save_name+'.npz', \n",
    "                        U=U, Y=Y, A=A, E=E, Wu=Au, Wy=Ay, Wa=Aa, Wv=Av,B=B)\n",
    "    return U, Y, A, E, Au, Ay, Aa, Av,B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished count ['1', '2', '3', '4', '5', '6']\n",
      "Now : 7\n",
      "Iteraction: 0\n",
      "total_loss:----------------- [[0.7413095]]\n",
      "train_auc:------------------- 0.7853571428571429\n",
      "time: 81.04380989074707  sec\n",
      "Iteraction: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\NTU\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3326, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-9-b47a1d578c10>\", line 184, in <module>\n",
      "    Ur, Yr, Ar, Er, Aur, Ayr, Aar, Avr,Br = training(name+try_i+'_Edims200')\n",
      "  File \"<ipython-input-8-a671369f0222>\", line 67, in training\n",
      "    image_i:image_1,image_j:image_2})\n",
      "  File \"C:\\Users\\NTU\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 956, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"C:\\Users\\NTU\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 1180, in _run\n",
      "    feed_dict_tensor, options, run_metadata)\n",
      "  File \"C:\\Users\\NTU\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 1359, in _do_run\n",
      "    run_metadata)\n",
      "  File \"C:\\Users\\NTU\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 1365, in _do_call\n",
      "    return fn(*args)\n",
      "  File \"C:\\Users\\NTU\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 1350, in _run_fn\n",
      "    target_list, run_metadata)\n",
      "  File \"C:\\Users\\NTU\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 1443, in _call_tf_sessionrun\n",
      "    run_metadata)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\NTU\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2040, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\NTU\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\NTU\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 319, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\NTU\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 353, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\NTU\\Anaconda3\\lib\\inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\NTU\\Anaconda3\\lib\\inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\NTU\\Anaconda3\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\NTU\\Anaconda3\\lib\\inspect.py\", line 739, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"C:\\Users\\NTU\\Anaconda3\\lib\\inspect.py\", line 708, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"C:\\Users\\NTU\\Anaconda3\\lib\\inspect.py\", line 693, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"C:\\Users\\NTU\\Anaconda3\\lib\\genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "par_weights = [0.01]\n",
    "beta_weights = [0.001]\n",
    "Embedding_weights = [0.01]\n",
    "Embedding_dims = [200]\n",
    "\n",
    "name = 'Cold_User_y57_'\n",
    "try_count = [str(try_i+1) for try_i in range(20)]\n",
    "\n",
    "testcount = 0\n",
    "finish_list = []\n",
    "for try_i in try_count:\n",
    "    for pary_weight in par_weights:\n",
    "        for beta_weight in beta_weights:\n",
    "            for Embedding_weight in Embedding_weights:\n",
    "                for embedding_dims in Embedding_dims:\n",
    "                    clear_output()\n",
    "                    print('Finished count',finish_list)\n",
    "                    finish_list.append(try_i)\n",
    "                    print('Now :',try_i)\n",
    "                    \"\"\"\n",
    "                    n: the number of users\n",
    "                    m: the number of YouTubers\n",
    "                    k: latent dims\n",
    "                    l: feature dims\n",
    "                    \"\"\"\n",
    "                    tf.reset_default_graph()\n",
    "\n",
    "                    user = tf.placeholder(tf.int32,shape=(1,))\n",
    "                    i = tf.placeholder(tf.int32, shape=(1,))\n",
    "                    j = tf.placeholder(tf.int32, shape=(1,))\n",
    "\n",
    "                    #多少個auxliary \n",
    "                    xf = tf.placeholder(tf.float32, shape=(None,l))\n",
    "                    l_id = tf.placeholder(tf.int32, shape=(None,))\n",
    "                    l_id_len = tf.placeholder(tf.int32,shape=(1,))\n",
    "                    positive_id = tf.placeholder(tf.int32, shape=(None,))\n",
    "                    positive_len = tf.placeholder(tf.int32,shape=(1,))\n",
    "                    r = tf.placeholder(tf.float32,shape=(None,))\n",
    "\n",
    "\n",
    "                    image_i = tf.placeholder(tf.float32, shape=(1,l))\n",
    "                    image_j = tf.placeholder(tf.float32, shape=(1,l))\n",
    "\n",
    "                    with tf.variable_scope(\"item_level\"):\n",
    "                        user_latent = tf.get_variable(\"user_latent\", [n, k],\n",
    "                                                              initializer=tf.random_normal_initializer(0,0.1,seed=3))\n",
    "                        item_latent = tf.get_variable(\"item_latent\", [m, k],\n",
    "                                                              initializer=tf.random_normal_initializer(0,0.1,seed=3)) \n",
    "                        aux_item = tf.get_variable(\"aux_item\", [m, k],\n",
    "                                                              initializer=tf.random_normal_initializer(0,0.1,seed=3))\n",
    "                        Wu = tf.get_variable(\"Wu\", [n,m,k],  \n",
    "                                                              initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Wy = tf.get_variable(\"Wy\", [n,m,k],   \n",
    "                                                             initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Wa = tf.get_variable(\"Wa\", [n,m,k],  \n",
    "                                                             initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Wv = tf.get_variable(\"Wv\", [n,m,embedding_dims],  \n",
    "                                                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        #Wve =  tf.get_variable(\"Wve\", [embedding_dims,l],  \n",
    "                        #                                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "                        #每個user 對於每個YouTuber都有一個權重\n",
    "                        #w1拿掉，wu\n",
    "                        #hyper?\n",
    "\n",
    "                        aux_new = tf.get_variable(\"aux_new\", [1,k], initializer=tf.constant_initializer(0.0))\n",
    "                        ########## Error part, how to get auxisize dynamically\n",
    "                        ####aux_size= tf.get_variable(name='aux_size', initializer=l_id.get_shape().as_list()[-1])\n",
    "\n",
    "                    with tf.variable_scope('feature_level'):\n",
    "                        embedding = tf.get_variable(\"embedding\", [embedding_dims,l],\n",
    "                                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Beta = tf.get_variable(\"beta\", [n,embedding_dims],\n",
    "                                    initializer=tf.random_normal_initializer(0.01,0.001,seed=10))\n",
    "\n",
    "                    #lookup the latent factors by user and id\n",
    "                    u = tf.nn.embedding_lookup(user_latent, user) #(1*k) 第幾個user latent factor\n",
    "                    vi = tf.nn.embedding_lookup(item_latent, i) \n",
    "                    vj = tf.nn.embedding_lookup(item_latent, j)\n",
    "\n",
    "\n",
    "                    wu = tf.squeeze(tf.nn.embedding_lookup(Wu, user)) #(m*k)\n",
    "                    wy = tf.squeeze(tf.nn.embedding_lookup(Wy, user)) #(m*k)\n",
    "                    wa = tf.squeeze(tf.nn.embedding_lookup(Wa, user)) #(m*k)\n",
    "                    wv = tf.squeeze(tf.nn.embedding_lookup(Wv, user)) #(m,l)\n",
    "                    beta = tf.nn.embedding_lookup(Beta, user) #user feature latent factor\n",
    "\n",
    "\n",
    "                    a_list=tf.Variable([])\n",
    "                    q = tf.constant(0)\n",
    "                    def att_cond(q,a_list):\n",
    "                        return tf.less(q,l_id_len[0])\n",
    "                    def att_body(q,a_list):\n",
    "                        xfi = tf.expand_dims(xf[q],0) #(1,l)\n",
    "                        wuui = tf.expand_dims(tf.nn.embedding_lookup(wu,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        wyui = tf.expand_dims(tf.nn.embedding_lookup(wy,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        waui = tf.expand_dims(tf.nn.embedding_lookup(wa,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        wvui = tf.expand_dims(tf.nn.embedding_lookup(wv,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        a_list = tf.concat([a_list,[(tf.nn.relu( tf.matmul(wuui, u, transpose_b=True) +\n",
    "                                tf.matmul(wyui, tf.expand_dims(tf.nn.embedding_lookup(item_latent,l_id[q]),0), transpose_b=True) +\n",
    "                                tf.matmul(waui, tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0), transpose_b=True) +\n",
    "                                tf.matmul(wvui,tf.matmul(embedding,xfi, transpose_b=True)))[0][0])*r[q]]],0)\n",
    "                        q += 1\n",
    "                        return q,  a_list\n",
    "\n",
    "                    _, a_list = tf.while_loop(att_cond,att_body,[q,a_list],shape_invariants=[q.get_shape(),tf.TensorShape([None])])\n",
    "\n",
    "                    # for while for smoothing\n",
    "                    #a_list_soft=tf.nn.softmax(a_list)\n",
    "                    a_list_smooth = tf.add(a_list,0.0000000001)\n",
    "                    a_list_soft = tf.divide(a_list_smooth,tf.reduce_sum(a_list_smooth, 0)) #without softmax\n",
    "\n",
    "                    norm_par = [wu,wy,wa,wv]\n",
    "\n",
    "                    wuui = tf.expand_dims(tf.nn.embedding_lookup(wu,l_id[-1]),0)\n",
    "                    wyui = tf.expand_dims(tf.nn.embedding_lookup(wy,l_id[-1]),0)\n",
    "                    waui = tf.expand_dims(tf.nn.embedding_lookup(wa,l_id[-1]),0)\n",
    "                    wvui = tf.expand_dims(tf.nn.embedding_lookup(wv,l_id[-1]),0)\n",
    "                    wu_be_relu = tf.matmul(wuui, u, transpose_b=True)\n",
    "                    wy_be_relu = tf.matmul(wyui, tf.expand_dims(tf.nn.embedding_lookup(item_latent,l_id[-1]),0), transpose_b=True)\n",
    "                    wa_be_relu = tf.matmul(waui, tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[-1]),0), transpose_b=True)\n",
    "                    wv_be_relu = tf.matmul(wvui, tf.matmul(embedding,tf.expand_dims(xf[-1],0), transpose_b=True))\n",
    "                    last_be_relu = [wu_be_relu,wy_be_relu,wa_be_relu,wv_be_relu]\n",
    "\n",
    "                    aux_np = tf.expand_dims(tf.zeros(k),0) #dimension (1,32)\n",
    "                    q = tf.constant(0)\n",
    "                    def sum_att_cond(q,aux_np):\n",
    "                        return tf.less(q,l_id_len[0])\n",
    "\n",
    "                    def sum_att_body(q,aux_np):\n",
    "                        #aux_np+=a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)\n",
    "                        aux_np = tf.math.add_n([aux_np,a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)]) \n",
    "                        q += 1\n",
    "                        return q, aux_np\n",
    "\n",
    "                    _,aux_np = tf.while_loop(sum_att_cond,sum_att_body,[q,aux_np])\n",
    "\n",
    "                    \"\"\"\n",
    "                    for q in range(3): #取q個auxliary item\n",
    "                        aux_np+=a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)\n",
    "                    \"\"\"\n",
    "\n",
    "                    aux_part = tf.matmul(aux_np, vi, transpose_b=True)\n",
    "                    #tf.print('aux attention:',aux_np)\n",
    "                    aux_np+=u #user_latent factor + sum (alpha*auxilary)\n",
    "                    aux_new=tf.assign(aux_new,aux_np) #把aux_new 的 值變成aux_np\n",
    "\n",
    "\n",
    "                    latent_i_part = tf.matmul(aux_new, vi, transpose_b=True)\n",
    "                    feature_i_part = tf.matmul(beta,(tf.matmul(embedding,image_i, transpose_b=True)))\n",
    "                    latent_j_part = tf.matmul(aux_new, vj, transpose_b=True)\n",
    "                    feature_j_part = tf.matmul(beta,(tf.matmul(embedding,image_j, transpose_b=True)))\n",
    "                    only_aux_i_part = tf.matmul(aux_np, vi, transpose_b=True)\n",
    "                    only_aux_j_part = tf.matmul(aux_np, vj, transpose_b=True)\n",
    "\n",
    "                    #矩陣中對應函數各自相乘\n",
    "                    # ex: tf.matmul(thetav,(tf.matmul(embedding, image_i, transpose_b=True)))\n",
    "                    xui = tf.matmul(aux_new, vi, transpose_b=True)+ tf.matmul(beta,(tf.matmul(embedding,image_i, transpose_b=True)))\n",
    "                    xuj = tf.matmul(aux_new, vj, transpose_b=True)+ tf.matmul(beta,(tf.matmul(embedding,image_j, transpose_b=True)))\n",
    "\n",
    "                    xuij = tf.subtract(xui,xuj)\n",
    "\n",
    "\n",
    "                    l2_norm = tf.add_n([\n",
    "                                0.0001 * tf.reduce_sum(tf.multiply(u, u)),\n",
    "                                0.0001 * tf.reduce_sum(tf.multiply(vi, vi)),\n",
    "                                0.0001 * tf.reduce_sum(tf.multiply(vj, vj)),\n",
    "\n",
    "\n",
    "                                0.01 * tf.reduce_sum(tf.multiply(wu, wu)),\n",
    "                                pary_weight * tf.reduce_sum(tf.multiply(wy, wy)),\n",
    "                                pary_weight * tf.reduce_sum(tf.multiply(wa, wa)),\n",
    "                                pary_weight * tf.reduce_sum(tf.multiply(wv,wv)),\n",
    "\n",
    "                                beta_weight * tf.reduce_sum(tf.multiply(beta,beta)),\n",
    "                                Embedding_weight * tf.reduce_sum(tf.multiply(embedding,embedding)),\n",
    "\n",
    "                                ])\n",
    "\n",
    "                    loss = l2_norm -tf.log(tf.sigmoid(xuij)) # objective funtion\n",
    "                    train_op = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(loss) #parameter optimize \n",
    "                    auc = tf.reduce_mean(tf.to_float(xuij > 0))\n",
    "\n",
    "                    Ur, Yr, Ar, Er, Aur, Ayr, Aar, Avr,Br = training(name+try_i+'_Edims200')\n",
    "                    print('Finish:,',try_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VBPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visual feature shape: (57, 2048)\n"
     ]
    }
   ],
   "source": [
    "image_f = np.load('image_2048D_y57.npy')\n",
    "visual_feature = image_f\n",
    "print('visual feature shape:',visual_feature.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(user_following)\n",
    "m = yt_num\n",
    "k = 64\n",
    "l = visual_feature.shape[1]\n",
    "embedding_dims = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(save_name): \n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    "    loss_acc_list = []\n",
    "    t0=time.time()\n",
    "    \n",
    "    train_yes_id=[] \n",
    "    for q in range(7):\n",
    "        print('Iteraction:',q)\n",
    "        train_auc=0\n",
    "        total_loss=0\n",
    "        xuij_auc=0\n",
    "        length = 0\n",
    "        for z in range(n):\n",
    "            \"\"\"\n",
    "            yes 用來存放選擇到的YouTuber feature (for auxilary)\n",
    "            yesr 用來存放user對該YouTuber的喜好程度(user_category 跟 YouTuber_category的相似性)\n",
    "            r_3 用來存放user 對該YouTuber種類的偏好(取max)\n",
    "            \"\"\"\n",
    "        \n",
    "            #取positive \n",
    "            train_t_sample = random.sample(train_t[z],len(train_t[z]))\n",
    "            #print('number of positive feedback', len(train_t_sample))\n",
    "        \n",
    "            #train_f_sample = random.sample(train_f[z],20)\n",
    "            for ta in train_t_sample:\n",
    "                #pos = sample.index(ta)\n",
    "                \n",
    "                image_1=np.expand_dims(visual_feature[ta],0) #(1,2048)\n",
    "                train_f_sample = random.sample(train_f[z],10)\n",
    "                \n",
    "                for b in train_f_sample:\n",
    "                    image_2=np.expand_dims(visual_feature[b],0) #(1,2048)\n",
    "                    _auc, _loss,_=sess.run([auc,loss,train_op], feed_dict={user: [z],\n",
    "                                        i: [ta], j: [b],image_i:image_1,image_j:image_2})\n",
    "\n",
    "                    train_auc+=_auc\n",
    "                    total_loss+=_loss\n",
    "                    length += 1\n",
    "\n",
    "        print(\"total_loss:-----------------\", total_loss/length)\n",
    "        print(\"train_auc:-------------------\", train_auc/length)\n",
    "        loss_acc_list.append([total_loss/length,train_auc/length,time.time()-t0])\n",
    "        print('time:',time.time()-t0,' sec')\n",
    "    print('Total cost ',time.time()-t0,' sec')   \n",
    "    U, Y, E,B =sess.run([user_latent, item_latent, embedding,Beta])\n",
    "    np.savez('./Result/Cold_user/'+save_name+'.npz', \n",
    "                        U=U, Y=Y, E=E,B=B)\n",
    "    return U, Y, E, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Dims [200]\n",
      "Now Dims: 200\n",
      "Iteraction: 0\n",
      "total_loss:----------------- [[0.74030656]]\n",
      "train_auc:------------------- 0.7960714285714285\n",
      "time: 9.250927925109863  sec\n",
      "Iteraction: 1\n",
      "total_loss:----------------- [[0.4163654]]\n",
      "train_auc:------------------- 0.8475\n",
      "time: 18.40341806411743  sec\n",
      "Iteraction: 2\n",
      "total_loss:----------------- [[0.38042802]]\n",
      "train_auc:------------------- 0.8691071428571429\n",
      "time: 27.616264581680298  sec\n",
      "Iteraction: 3\n",
      "total_loss:----------------- [[0.34230784]]\n",
      "train_auc:------------------- 0.8867857142857143\n",
      "time: 36.837541818618774  sec\n",
      "Iteraction: 4\n",
      "total_loss:----------------- [[0.32055247]]\n",
      "train_auc:------------------- 0.9026785714285714\n",
      "time: 46.14727807044983  sec\n",
      "Iteraction: 5\n",
      "total_loss:----------------- [[0.29446474]]\n",
      "train_auc:------------------- 0.9198214285714286\n",
      "time: 55.35170292854309  sec\n",
      "Iteraction: 6\n",
      "total_loss:----------------- [[0.28744182]]\n",
      "train_auc:------------------- 0.9182142857142858\n",
      "time: 64.61437654495239  sec\n",
      "Total cost  64.61437654495239  sec\n",
      "Finish:, 1\n"
     ]
    }
   ],
   "source": [
    "par_weights = [0.01]\n",
    "beta_weights = [0.001]\n",
    "Embedding_weights = [0.01]\n",
    "Embedding_dims = [200]\n",
    "\n",
    "name = 'Cold_User_y57_VBPR'\n",
    "try_count = [str(try_i) for try_i in range(2)]\n",
    "\n",
    "testcount = 0\n",
    "finish_list = []\n",
    "for try_i in try_count:\n",
    "    for pary_weight in par_weights:\n",
    "        for beta_weight in beta_weights:\n",
    "            for Embedding_weight in Embedding_weights:\n",
    "                for embedding_dims in Embedding_dims:\n",
    "                    clear_output()\n",
    "                    print('Finished Dims',finish_list)\n",
    "                    finish_list.append(embedding_dims)\n",
    "                    print('Now Dims:',embedding_dims)\n",
    "                    \"\"\"\n",
    "                    n: the number of users\n",
    "                    m: the number of YouTubers\n",
    "                    k: latent dims\n",
    "                    l: feature dims\n",
    "                    \"\"\"\n",
    "                    tf.reset_default_graph()\n",
    "\n",
    "                    user = tf.placeholder(tf.int32,shape=(1,))\n",
    "                    i = tf.placeholder(tf.int32, shape=(1,))\n",
    "                    j = tf.placeholder(tf.int32, shape=(1,))\n",
    "\n",
    "                    #多少個auxliary \n",
    "\n",
    "                    image_i = tf.placeholder(tf.float32, shape=(1,l))\n",
    "                    image_j = tf.placeholder(tf.float32, shape=(1,l))\n",
    "\n",
    "                    with tf.variable_scope(\"item_level\"):\n",
    "                        user_latent = tf.get_variable(\"user_latent\", [n, k],\n",
    "                                                              initializer=tf.random_normal_initializer(0,0.1,seed=3))\n",
    "                        item_latent = tf.get_variable(\"item_latent\", [m, k],\n",
    "                                                              initializer=tf.random_normal_initializer(0,0.1,seed=3)) \n",
    "                        #aux_item = tf.get_variable(\"aux_item\", [m, k],\n",
    "                        #                                      initializer=tf.random_normal_initializer(0,0.1,seed=3))\n",
    "                       \n",
    "                        #Wve =  tf.get_variable(\"Wve\", [embedding_dims,l],  \n",
    "                        #                                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "                        #每個user 對於每個YouTuber都有一個權重\n",
    "                        #w1拿掉，wu\n",
    "                        #hyper?\n",
    "\n",
    "                        aux_new = tf.get_variable(\"aux_new\", [1,k], initializer=tf.constant_initializer(0.0))\n",
    "                        ########## Error part, how to get auxisize dynamically\n",
    "                        ####aux_size= tf.get_variable(name='aux_size', initializer=l_id.get_shape().as_list()[-1])\n",
    "\n",
    "                    with tf.variable_scope('feature_level'):\n",
    "                        embedding = tf.get_variable(\"embedding\", [embedding_dims,l],\n",
    "                                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Beta = tf.get_variable(\"beta\", [n,embedding_dims],\n",
    "                                    initializer=tf.random_normal_initializer(0.01,0.001,seed=10))\n",
    "\n",
    "                    #lookup the latent factors by user and id\n",
    "                    u = tf.nn.embedding_lookup(user_latent, user) #(1*k) 第幾個user latent factor\n",
    "                    vi = tf.nn.embedding_lookup(item_latent, i) \n",
    "                    vj = tf.nn.embedding_lookup(item_latent, j)\n",
    "\n",
    "                    beta = tf.nn.embedding_lookup(Beta, user) #user feature latent factor\n",
    "\n",
    "                    \n",
    "                    aux_new=tf.assign(aux_new,u) #把aux_new 的 值變成aux_np\n",
    "                    \n",
    "                    \n",
    "                    #矩陣中對應函數各自相乘\n",
    "                    # ex: tf.matmul(thetav,(tf.matmul(embedding, image_i, transpose_b=True)))\n",
    "                    xui = tf.matmul(aux_new, vi, transpose_b=True)+ tf.matmul(beta,(tf.matmul(embedding,image_i, transpose_b=True)))\n",
    "                    xuj = tf.matmul(aux_new, vj, transpose_b=True)+ tf.matmul(beta,(tf.matmul(embedding,image_j, transpose_b=True)))\n",
    "\n",
    "                    xuij = tf.subtract(xui,xuj)\n",
    "\n",
    "\n",
    "                    l2_norm = tf.add_n([\n",
    "                                0.0001 * tf.reduce_sum(tf.multiply(u, u)),\n",
    "                                0.0001 * tf.reduce_sum(tf.multiply(vi, vi)),\n",
    "                                0.0001 * tf.reduce_sum(tf.multiply(vj, vj)),\n",
    "                                beta_weight * tf.reduce_sum(tf.multiply(beta,beta)),\n",
    "                                Embedding_weight * tf.reduce_sum(tf.multiply(embedding,embedding)),\n",
    "\n",
    "                                ])\n",
    "\n",
    "                    loss = l2_norm -tf.log(tf.sigmoid(xuij)) # objective funtion\n",
    "                    train_op = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(loss) #parameter optimize \n",
    "                    auc = tf.reduce_mean(tf.to_float(xuij > 0))\n",
    "\n",
    "                    Ur, Yr,Er,Br = training(name+try_i+'_Edims200')\n",
    "                    print('Finish:,',try_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ACF_NR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_3374 = np.load('image_2048D_y57.npy')\n",
    "all_auxilary = [i for i in range(yt_num)]\n",
    "n = len(user_following)\n",
    "m = yt_num  \n",
    "k = 64\n",
    "l = all_3374.shape[1]\n",
    "#embedding_dims = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(save_name): \n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    "    loss_acc_list = []\n",
    "    t0=time.time()\n",
    "    \n",
    "    train_yes_id=[] \n",
    "    for q in range(7):\n",
    "        print('Iteraction:',q)\n",
    "        train_auc=0\n",
    "        total_loss=0\n",
    "        xuij_auc=0\n",
    "        length = 0\n",
    "        for z in range(n):\n",
    "            \"\"\"\n",
    "            yes 用來存放選擇到的YouTuber feature (for auxilary)\n",
    "            \"\"\"\n",
    "            yes=[]\n",
    "            yesr=[]\n",
    "        \n",
    "            \"\"\"\n",
    "            only choose positive \n",
    "            \"\"\"\n",
    "            #sample=random.sample(train_t[z],len(train_t[z])) #選全部的Positive\n",
    "            \"\"\"\n",
    "            choose all YouTuber \n",
    "            \"\"\"\n",
    "            sample= all_auxilary #選全部的Positive\n",
    "        \n",
    "            #sample=random.sample(train_t[z]+train_f[z],len(train_t[z])+len(train_f[z]))\n",
    "        \n",
    "            \n",
    "         \n",
    "            for b in range(len(sample)):\n",
    "                yes.append(all_3374[sample[b]])\n",
    "        \n",
    "            yes=np.array(yes)\n",
    "        \n",
    "            #取positive \n",
    "            train_t_sample = random.sample(train_t[z],len(train_t[z]))\n",
    "            #print('number of positive feedback', len(train_t_sample))\n",
    "        \n",
    "            #train_f_sample = random.sample(train_f[z],20)\n",
    "            for ta in train_t_sample:\n",
    "                pos = sample.index(ta)\n",
    "                \n",
    "                image_1=np.expand_dims(all_3374[ta],0) #(1,2048)\n",
    "                train_f_sample = random.sample(train_f[z],10)\n",
    "                \n",
    "                for b in train_f_sample:\n",
    "                    image_2=np.expand_dims(all_3374[b],0) #(1,2048)\n",
    "                    _a_list,r3,_auc, _loss,_=sess.run([a_list_smooth,a_list_soft,auc,loss,train_op], feed_dict={user: [z],\n",
    "                                        i: [ta], j: [b], xf: yes , l_id:sample, l_id_len:[len(sample)],positive_id:train_t[z],positive_len:[len(train_t[z])],\n",
    "                                        image_i:image_1,image_j:image_2})\n",
    "                    #print(_a_list)\n",
    "                    #print(r3)\n",
    "                    train_auc+=_auc\n",
    "                    total_loss+=_loss\n",
    "                    length += 1\n",
    "        #print('a_list:',_a_list)\n",
    "        #print('a_list_soft:',r3)\n",
    "        print(\"total_loss:-----------------\", total_loss/length)\n",
    "        print(\"train_auc:-------------------\", train_auc/length)\n",
    "        loss_acc_list.append([total_loss/length,train_auc/length,time.time()-t0])\n",
    "        print('time:',time.time()-t0,' sec')\n",
    "    print('Total cost ',time.time()-t0,' sec')   \n",
    "    U, Y, A,E,Au, Ay, Aa, Av =sess.run([user_latent, item_latent, aux_item, embedding,Wu, Wy, Wa, Wv])\n",
    "    np.savez('./Result/Cold_user/'+save_name+'.npz', \n",
    "                        U=U, Y=Y, A=A,E=E,Wu=Au, Wy=Ay, Wa=Aa, Wv=Av)\n",
    "    return U, Y, A, E, Au, Ay, Aa, Av"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Img0\n",
      "Iteraction: 0\n",
      "total_loss:----------------- [[0.7985729]]\n",
      "train_auc:------------------- 0.5132142857142857\n",
      "time: 72.67318964004517  sec\n",
      "Iteraction: 1\n",
      "total_loss:----------------- [[0.67502016]]\n",
      "train_auc:------------------- 0.6541071428571429\n",
      "time: 145.1907103061676  sec\n",
      "Iteraction: 2\n",
      "total_loss:----------------- [[0.66097075]]\n",
      "train_auc:------------------- 0.7717857142857143\n",
      "time: 217.63529133796692  sec\n",
      "Iteraction: 3\n",
      "total_loss:----------------- [[0.65200603]]\n",
      "train_auc:------------------- 0.8239285714285715\n",
      "time: 290.9899892807007  sec\n",
      "Iteraction: 4\n",
      "total_loss:----------------- [[0.64435595]]\n",
      "train_auc:------------------- 0.8446428571428571\n",
      "time: 364.67226481437683  sec\n",
      "Iteraction: 5\n",
      "total_loss:----------------- [[0.63485426]]\n",
      "train_auc:------------------- 0.84625\n",
      "time: 438.97003626823425  sec\n",
      "Iteraction: 6\n",
      "total_loss:----------------- [[0.6265901]]\n",
      "train_auc:------------------- 0.8371428571428572\n",
      "time: 513.3413581848145  sec\n",
      "Total cost  513.3413581848145  sec\n",
      "Finish : Img0\n",
      "Img1\n",
      "Iteraction: 0\n",
      "total_loss:----------------- [[0.7999569]]\n",
      "train_auc:------------------- 0.5082142857142857\n",
      "time: 72.57942724227905  sec\n",
      "Iteraction: 1\n",
      "total_loss:----------------- [[0.674683]]\n",
      "train_auc:------------------- 0.6555357142857143\n",
      "time: 144.80787348747253  sec\n",
      "Iteraction: 2\n",
      "total_loss:----------------- [[0.662033]]\n",
      "train_auc:------------------- 0.7667857142857143\n",
      "time: 216.630197763443  sec\n",
      "Iteraction: 3\n",
      "total_loss:----------------- [[0.65219647]]\n",
      "train_auc:------------------- 0.8241071428571428\n",
      "time: 287.18006014823914  sec\n",
      "Iteraction: 4\n",
      "total_loss:----------------- [[0.64428914]]\n",
      "train_auc:------------------- 0.8482142857142857\n",
      "time: 357.8700180053711  sec\n",
      "Iteraction: 5\n",
      "total_loss:----------------- [[0.635683]]\n",
      "train_auc:------------------- 0.8464285714285714\n",
      "time: 428.8354551792145  sec\n",
      "Iteraction: 6\n",
      "total_loss:----------------- [[0.62603235]]\n",
      "train_auc:------------------- 0.8360714285714286\n",
      "time: 499.736780166626  sec\n",
      "Total cost  499.736780166626  sec\n",
      "Finish : Img1\n"
     ]
    }
   ],
   "source": [
    "par_weights = [0.01]\n",
    "beta_weights = [0.001]\n",
    "Embedding_weights = [0.01]\n",
    "Embedding_dims = [200]\n",
    "\n",
    "name = 'Cold_User_y57_ACF_NR'\n",
    "count_try = ['Img'+str(i) for i in range(2)]\n",
    "\n",
    "testcount = 0\n",
    "finish_list = []\n",
    "for try_i in count_try:\n",
    "    for pary_weight in par_weights:\n",
    "        for beta_weight in beta_weights:\n",
    "            for Embedding_weight in Embedding_weights:\n",
    "                for embedding_dims in Embedding_dims:\n",
    "                    #clear_output()\n",
    "                    #print('Finished Dims',finish_list)\n",
    "                    finish_list.append(embedding_dims)\n",
    "                    #print('Now Dims:',embedding_dims)\n",
    "                    print(try_i)\n",
    "                    \"\"\"\n",
    "                    n: the number of users\n",
    "                    m: the number of YouTubers\n",
    "                    k: latent dims\n",
    "                    l: feature dims\n",
    "                    \"\"\"\n",
    "                    tf.reset_default_graph()\n",
    "\n",
    "                    user = tf.placeholder(tf.int32,shape=(1,))\n",
    "                    i = tf.placeholder(tf.int32, shape=(1,))\n",
    "                    j = tf.placeholder(tf.int32, shape=(1,))\n",
    "\n",
    "                    #多少個auxliary \n",
    "                    xf = tf.placeholder(tf.float32, shape=(None,l))\n",
    "                    l_id = tf.placeholder(tf.int32, shape=(None,))\n",
    "                    l_id_len = tf.placeholder(tf.int32,shape=(1,))\n",
    "                    positive_id = tf.placeholder(tf.int32, shape=(None,))\n",
    "                    positive_len = tf.placeholder(tf.int32,shape=(1,))\n",
    "\n",
    "\n",
    "                    image_i = tf.placeholder(tf.float32, shape=(1,l))\n",
    "                    image_j = tf.placeholder(tf.float32, shape=(1,l))\n",
    "\n",
    "                    with tf.variable_scope(\"item_level\"):\n",
    "                        user_latent = tf.get_variable(\"user_latent\", [n, k],\n",
    "                                                              initializer=tf.random_normal_initializer(0,0.1,seed=3))\n",
    "                        item_latent = tf.get_variable(\"item_latent\", [m, k],\n",
    "                                                              initializer=tf.random_normal_initializer(0,0.1,seed=3)) \n",
    "                        aux_item = tf.get_variable(\"aux_item\", [m, k],\n",
    "                                                              initializer=tf.random_normal_initializer(0,0.1,seed=3))\n",
    "                        Wu = tf.get_variable(\"Wu\", [n,m,k],  \n",
    "                                                              initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Wy = tf.get_variable(\"Wy\", [n,m,k],   \n",
    "                                                             initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Wa = tf.get_variable(\"Wa\", [n,m,k],  \n",
    "                                                             initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Wv = tf.get_variable(\"Wv\", [n,m,embedding_dims],  \n",
    "                                                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        #Wve =  tf.get_variable(\"Wve\", [embedding_dims,l],  \n",
    "                        #                                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "                        with tf.variable_scope('feature_level'):\n",
    "                            embedding = tf.get_variable(\"embedding\", [embedding_dims,l],\n",
    "                                        initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "                        aux_new = tf.get_variable(\"aux_new\", [1,k], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "\n",
    "\n",
    "                    #lookup the latent factors by user and id\n",
    "                    u = tf.nn.embedding_lookup(user_latent, user) #(1*k) 第幾個user latent factor\n",
    "                    vi = tf.nn.embedding_lookup(item_latent, i) \n",
    "                    vj = tf.nn.embedding_lookup(item_latent, j)\n",
    "\n",
    "\n",
    "                    wu = tf.squeeze(tf.nn.embedding_lookup(Wu, user)) #(m*k)\n",
    "                    wy = tf.squeeze(tf.nn.embedding_lookup(Wy, user)) #(m*k)\n",
    "                    wa = tf.squeeze(tf.nn.embedding_lookup(Wa, user)) #(m*k)\n",
    "                    wv = tf.squeeze(tf.nn.embedding_lookup(Wv, user)) #(m,l)\n",
    "\n",
    "\n",
    "                    a_list=tf.Variable([])\n",
    "                    q = tf.constant(0)\n",
    "                    def att_cond(q,a_list):\n",
    "                        return tf.less(q,l_id_len[0])\n",
    "                    def att_body(q,a_list):\n",
    "                        xfi = tf.expand_dims(xf[q],0) #(1,l)\n",
    "                        wuui = tf.expand_dims(tf.nn.embedding_lookup(wu,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        wyui = tf.expand_dims(tf.nn.embedding_lookup(wy,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        waui = tf.expand_dims(tf.nn.embedding_lookup(wa,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        wvui = tf.expand_dims(tf.nn.embedding_lookup(wv,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        a_list = tf.concat([a_list,[(tf.nn.relu( tf.matmul(wuui, u, transpose_b=True) +\n",
    "                                tf.matmul(wyui, tf.expand_dims(tf.nn.embedding_lookup(item_latent,l_id[q]),0), transpose_b=True) +\n",
    "                                tf.matmul(waui, tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0), transpose_b=True) +\n",
    "                                tf.matmul(wvui,tf.matmul(embedding,xfi, transpose_b=True)))[0][0])]],0)\n",
    "                        q += 1\n",
    "                        return q,  a_list\n",
    "\n",
    "                    _, a_list = tf.while_loop(att_cond,att_body,[q,a_list],shape_invariants=[q.get_shape(),tf.TensorShape([None])])\n",
    "\n",
    "                    # for while for smoothing\n",
    "                    #a_list_soft=tf.nn.softmax(a_list)\n",
    "                    a_list_smooth = tf.add(a_list,0.0000000001)\n",
    "                    a_list_soft = tf.divide(a_list_smooth,tf.reduce_sum(a_list_smooth, 0)) #without softmax\n",
    "\n",
    "                    aux_np = tf.expand_dims(tf.zeros(k),0) #dimension (1,32)\n",
    "                    q = tf.constant(0)\n",
    "                    def sum_att_cond(q,aux_np):\n",
    "                        return tf.less(q,l_id_len[0])\n",
    "\n",
    "                    def sum_att_body(q,aux_np):\n",
    "                        #aux_np+=a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)\n",
    "                        aux_np = tf.math.add_n([aux_np,a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)]) \n",
    "                        q += 1\n",
    "                        return q, aux_np\n",
    "\n",
    "                    _,aux_np = tf.while_loop(sum_att_cond,sum_att_body,[q,aux_np])\n",
    "\n",
    "                    \"\"\"\n",
    "                    for q in range(3): #取q個auxliary item\n",
    "                        aux_np+=a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)\n",
    "                    \"\"\"\n",
    "\n",
    "                    #tf.print('aux attention:',aux_np)\n",
    "                    aux_np+=u #user_latent factor + sum (alpha*auxilary)\n",
    "                    aux_new=tf.assign(aux_new,aux_np) #把aux_new 的 值變成aux_np\n",
    "\n",
    "\n",
    "                    #矩陣中對應函數各自相乘\n",
    "                    # ex: tf.matmul(thetav,(tf.matmul(embedding, image_i, transpose_b=True)))\n",
    "                    xui = tf.matmul(aux_new, vi, transpose_b=True)\n",
    "                    xuj = tf.matmul(aux_new, vj, transpose_b=True)\n",
    "\n",
    "                    xuij = tf.subtract(xui,xuj)\n",
    "\n",
    "                    l2_norm = tf.add_n([\n",
    "                                0.0001 * tf.reduce_sum(tf.multiply(u, u)),\n",
    "                                0.0001 * tf.reduce_sum(tf.multiply(vi, vi)),\n",
    "                                0.0001 * tf.reduce_sum(tf.multiply(vj, vj)),\n",
    "\n",
    "\n",
    "                                0.01 * tf.reduce_sum(tf.multiply(wu, wu)),\n",
    "                                pary_weight * tf.reduce_sum(tf.multiply(wy, wy)),\n",
    "                                pary_weight * tf.reduce_sum(tf.multiply(wa, wa)),\n",
    "                                pary_weight * tf.reduce_sum(tf.multiply(wv,wv)),\n",
    "                                Embedding_weight * tf.reduce_sum(tf.multiply(embedding,embedding)),\n",
    "                                ])\n",
    "\n",
    "                    loss = l2_norm -tf.log(tf.sigmoid(xuij)) # objective funtion\n",
    "                    train_op = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(loss) #parameter optimize \n",
    "                    auc = tf.reduce_mean(tf.to_float(xuij > 0))\n",
    "\n",
    "                    Ur, Yr, Ar, Er, Aur, Ayr, Aar, Avr = training(name+try_i+'_Edims200')\n",
    "                    print('Finish :', try_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ACF_R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(save_name): \n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    "    loss_acc_list = []\n",
    "    t0=time.time()\n",
    "    \n",
    "    train_yes_id=[] \n",
    "    for q in range(7):\n",
    "        print('Iteraction:',q)\n",
    "        train_auc=0\n",
    "        total_loss=0\n",
    "        xuij_auc=0\n",
    "        length = 0\n",
    "        for z in range(n):\n",
    "            \"\"\"\n",
    "            yes 用來存放選擇到的YouTuber feature (for auxilary)\n",
    "            yesr 用來存放user對該YouTuber的喜好程度(user_category 跟 YouTuber_category的相似性)\n",
    "            r_3 用來存放user 對該YouTuber種類的偏好(取max)\n",
    "            \"\"\"\n",
    "            yes=[]\n",
    "            yesr=[]\n",
    "        \n",
    "            \"\"\"\n",
    "            only choose positive \n",
    "            \"\"\"\n",
    "            #sample=random.sample(train_t[z],len(train_t[z])) #選全部的Positive\n",
    "            \"\"\"\n",
    "            choose all YouTuber \n",
    "            \"\"\"\n",
    "            sample= all_auxilary #選全部的Positive\n",
    "        \n",
    "            #sample=random.sample(train_t[z]+train_f[z],len(train_t[z])+len(train_f[z]))\n",
    "        \n",
    "            #user degree of category favor \n",
    "            r_3=np.zeros(len(sample)) \n",
    "         \n",
    "            for b in range(len(sample)):\n",
    "                yes.append(all_3374[sample[b]])\n",
    "                yesr.append(YouTuber_category[sample[b]]*user_category_norm[z])\n",
    "        \n",
    "            for b in range(len(yesr)):\n",
    "                r_3[b]=max(yesr[b])\n",
    "            #print('r_3:',r_3)\n",
    "        \n",
    "            yes=np.array(yes)\n",
    "        \n",
    "            #取positive \n",
    "            train_t_sample = random.sample(train_t[z],len(train_t[z]))\n",
    "            #print('number of positive feedback', len(train_t_sample))\n",
    "        \n",
    "            #train_f_sample = random.sample(train_f[z],20)\n",
    "            for ta in train_t_sample:\n",
    "                pos = sample.index(ta)\n",
    "                \n",
    "                image_1=np.expand_dims(all_3374[ta],0) #(1,2048)\n",
    "                train_f_sample = random.sample(train_f[z],10)\n",
    "                \n",
    "                for b in train_f_sample:\n",
    "                    image_2=np.expand_dims(all_3374[b],0) #(1,2048)\n",
    "                    _a_list,r3,_auc, _loss,_=sess.run([a_list_smooth,a_list_soft,auc,loss,train_op], feed_dict={user: [z],\n",
    "                                        i: [ta], j: [b], xf: yes , l_id:sample, l_id_len:[len(sample)],positive_id:train_t[z],positive_len:[len(train_t[z])],r:r_3,\n",
    "                                        image_i:image_1,image_j:image_2})\n",
    "                    #print(_a_list)\n",
    "                    #print(r3)\n",
    "                    train_auc+=_auc\n",
    "                    total_loss+=_loss\n",
    "                    length += 1\n",
    "        #print('a_list:',_a_list)\n",
    "        #print('a_list_soft:',r3)\n",
    "        print(\"total_loss:-----------------\", total_loss/length)\n",
    "        print(\"train_auc:-------------------\", train_auc/length)\n",
    "        loss_acc_list.append([total_loss/length,train_auc/length,time.time()-t0])\n",
    "        print('time:',time.time()-t0,' sec')\n",
    "    print('Total cost ',time.time()-t0,' sec')   \n",
    "    U, Y, A,E,Au, Ay, Aa, Av =sess.run([user_latent, item_latent, aux_item, embedding,Wu, Wy, Wa, Wv])\n",
    "    np.savez('./Result/Cold_user/'+save_name+'.npz'+save_name+'.npz', \n",
    "                        U=U, Y=Y, A=A,E=E,Wu=Au, Wy=Ay, Wa=Aa, Wv=Av)\n",
    "    return U, Y, A, E, Au, Ay, Aa, Av"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Img0\n",
      "Iteraction: 0\n",
      "total_loss:----------------- [[0.78681135]]\n",
      "train_auc:------------------- 0.5869642857142857\n",
      "time: 73.03725576400757  sec\n",
      "Iteraction: 1\n",
      "total_loss:----------------- [[0.6587614]]\n",
      "train_auc:------------------- 0.7367857142857143\n",
      "time: 146.0402274131775  sec\n",
      "Iteraction: 2\n",
      "total_loss:----------------- [[0.64000213]]\n",
      "train_auc:------------------- 0.8103571428571429\n",
      "time: 220.05187320709229  sec\n",
      "Iteraction: 3\n",
      "total_loss:----------------- [[0.6238449]]\n",
      "train_auc:------------------- 0.85625\n",
      "time: 294.21029782295227  sec\n",
      "Iteraction: 4\n",
      "total_loss:----------------- [[0.60780925]]\n",
      "train_auc:------------------- 0.8746428571428572\n",
      "time: 368.2961769104004  sec\n",
      "Iteraction: 5\n",
      "total_loss:----------------- [[0.59339726]]\n",
      "train_auc:------------------- 0.8692857142857143\n",
      "time: 442.43669962882996  sec\n",
      "Iteraction: 6\n",
      "total_loss:----------------- [[0.5778219]]\n",
      "train_auc:------------------- 0.8721428571428571\n",
      "time: 518.198000907898  sec\n",
      "Total cost  518.198000907898  sec\n",
      "Finish :, Img0\n",
      "Img1\n",
      "Iteraction: 0\n",
      "total_loss:----------------- [[0.7864851]]\n",
      "train_auc:------------------- 0.5869642857142857\n",
      "time: 73.6879186630249  sec\n",
      "Iteraction: 1\n",
      "total_loss:----------------- [[0.65832514]]\n",
      "train_auc:------------------- 0.72375\n",
      "time: 146.68011808395386  sec\n",
      "Iteraction: 2\n",
      "total_loss:----------------- [[0.6412413]]\n",
      "train_auc:------------------- 0.8039285714285714\n",
      "time: 220.60715007781982  sec\n",
      "Iteraction: 3\n",
      "total_loss:----------------- [[0.6251209]]\n",
      "train_auc:------------------- 0.8533928571428572\n",
      "time: 295.0101408958435  sec\n",
      "Iteraction: 4\n",
      "total_loss:----------------- [[0.60887444]]\n",
      "train_auc:------------------- 0.8675\n",
      "time: 369.46739172935486  sec\n",
      "Iteraction: 5\n",
      "total_loss:----------------- [[0.5951783]]\n",
      "train_auc:------------------- 0.8692857142857143\n",
      "time: 443.8318600654602  sec\n",
      "Iteraction: 6\n",
      "total_loss:----------------- [[0.57848364]]\n",
      "train_auc:------------------- 0.8666071428571429\n",
      "time: 518.3712980747223  sec\n",
      "Total cost  518.3712980747223  sec\n",
      "Finish :, Img1\n"
     ]
    }
   ],
   "source": [
    "par_weights = [0.01]\n",
    "beta_weights = [0.001]\n",
    "Embedding_weights = [0.01]\n",
    "Embedding_dims = [200]\n",
    "\n",
    "name = 'Cold_User_y57_ACF_R'\n",
    "count_try = ['Img'+str(i) for i in range(2)]\n",
    "\n",
    "testcount = 0\n",
    "finish_list = []\n",
    "for try_i in count_try:\n",
    "    for pary_weight in par_weights:\n",
    "        for beta_weight in beta_weights:\n",
    "            for Embedding_weight in Embedding_weights:\n",
    "                for embedding_dims in Embedding_dims:\n",
    "                    #clear_output()\n",
    "                    #print('Finished Dims',finish_list)\n",
    "                    finish_list.append(embedding_dims)\n",
    "                    #print('Now Dims:',embedding_dims)\n",
    "                    print(try_i)\n",
    "                    \"\"\"\n",
    "                    n: the number of users\n",
    "                    m: the number of YouTubers\n",
    "                    k: latent dims\n",
    "                    l: feature dims\n",
    "                    \"\"\"\n",
    "                    tf.reset_default_graph()\n",
    "\n",
    "                    user = tf.placeholder(tf.int32,shape=(1,))\n",
    "                    i = tf.placeholder(tf.int32, shape=(1,))\n",
    "                    j = tf.placeholder(tf.int32, shape=(1,))\n",
    "\n",
    "                    #多少個auxliary \n",
    "                    xf = tf.placeholder(tf.float32, shape=(None,l))\n",
    "                    l_id = tf.placeholder(tf.int32, shape=(None,))\n",
    "                    l_id_len = tf.placeholder(tf.int32,shape=(1,))\n",
    "                    positive_id = tf.placeholder(tf.int32, shape=(None,))\n",
    "                    positive_len = tf.placeholder(tf.int32,shape=(1,))\n",
    "                    r = tf.placeholder(tf.float32,shape=(None,))\n",
    "\n",
    "\n",
    "                    image_i = tf.placeholder(tf.float32, shape=(1,l))\n",
    "                    image_j = tf.placeholder(tf.float32, shape=(1,l))\n",
    "\n",
    "                    with tf.variable_scope(\"item_level\"):\n",
    "                        user_latent = tf.get_variable(\"user_latent\", [n, k],\n",
    "                                                              initializer=tf.random_normal_initializer(0,0.1,seed=3))\n",
    "                        item_latent = tf.get_variable(\"item_latent\", [m, k],\n",
    "                                                              initializer=tf.random_normal_initializer(0,0.1,seed=3)) \n",
    "                        aux_item = tf.get_variable(\"aux_item\", [m, k],\n",
    "                                                              initializer=tf.random_normal_initializer(0,0.1,seed=3))\n",
    "                        Wu = tf.get_variable(\"Wu\", [n,m,k],  \n",
    "                                                              initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Wy = tf.get_variable(\"Wy\", [n,m,k],   \n",
    "                                                             initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Wa = tf.get_variable(\"Wa\", [n,m,k],  \n",
    "                                                             initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Wv = tf.get_variable(\"Wv\", [n,m,embedding_dims],  \n",
    "                                                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        #Wve =  tf.get_variable(\"Wve\", [embedding_dims,l],  \n",
    "                        #                                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "                        with tf.variable_scope('feature_level'):\n",
    "                            embedding = tf.get_variable(\"embedding\", [embedding_dims,l],\n",
    "                                        initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "                        aux_new = tf.get_variable(\"aux_new\", [1,k], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "\n",
    "\n",
    "                    #lookup the latent factors by user and id\n",
    "                    u = tf.nn.embedding_lookup(user_latent, user) #(1*k) 第幾個user latent factor\n",
    "                    vi = tf.nn.embedding_lookup(item_latent, i) \n",
    "                    vj = tf.nn.embedding_lookup(item_latent, j)\n",
    "\n",
    "\n",
    "                    wu = tf.squeeze(tf.nn.embedding_lookup(Wu, user)) #(m*k)\n",
    "                    wy = tf.squeeze(tf.nn.embedding_lookup(Wy, user)) #(m*k)\n",
    "                    wa = tf.squeeze(tf.nn.embedding_lookup(Wa, user)) #(m*k)\n",
    "                    wv = tf.squeeze(tf.nn.embedding_lookup(Wv, user)) #(m,l)\n",
    "\n",
    "\n",
    "                    a_list=tf.Variable([])\n",
    "                    q = tf.constant(0)\n",
    "                    def att_cond(q,a_list):\n",
    "                        return tf.less(q,l_id_len[0])\n",
    "                    def att_body(q,a_list):\n",
    "                        xfi = tf.expand_dims(xf[q],0) #(1,l)\n",
    "                        wuui = tf.expand_dims(tf.nn.embedding_lookup(wu,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        wyui = tf.expand_dims(tf.nn.embedding_lookup(wy,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        waui = tf.expand_dims(tf.nn.embedding_lookup(wa,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        wvui = tf.expand_dims(tf.nn.embedding_lookup(wv,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        a_list = tf.concat([a_list,[(tf.nn.relu( tf.matmul(wuui, u, transpose_b=True) +\n",
    "                                tf.matmul(wyui, tf.expand_dims(tf.nn.embedding_lookup(item_latent,l_id[q]),0), transpose_b=True) +\n",
    "                                tf.matmul(waui, tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0), transpose_b=True) +\n",
    "                                tf.matmul(wvui,tf.matmul(embedding,xfi, transpose_b=True)))[0][0])*r[q]]],0)\n",
    "                        q += 1\n",
    "                        return q,  a_list\n",
    "\n",
    "                    _, a_list = tf.while_loop(att_cond,att_body,[q,a_list],shape_invariants=[q.get_shape(),tf.TensorShape([None])])\n",
    "\n",
    "                    # for while for smoothing\n",
    "                    #a_list_soft=tf.nn.softmax(a_list)\n",
    "                    a_list_smooth = tf.add(a_list,0.0000000001)\n",
    "                    a_list_soft = tf.divide(a_list_smooth,tf.reduce_sum(a_list_smooth, 0)) #without softmax\n",
    "\n",
    "                    aux_np = tf.expand_dims(tf.zeros(k),0) #dimension (1,32)\n",
    "                    q = tf.constant(0)\n",
    "                    def sum_att_cond(q,aux_np):\n",
    "                        return tf.less(q,l_id_len[0])\n",
    "\n",
    "                    def sum_att_body(q,aux_np):\n",
    "                        #aux_np+=a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)\n",
    "                        aux_np = tf.math.add_n([aux_np,a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)]) \n",
    "                        q += 1\n",
    "                        return q, aux_np\n",
    "\n",
    "                    _,aux_np = tf.while_loop(sum_att_cond,sum_att_body,[q,aux_np])\n",
    "\n",
    "                    \"\"\"\n",
    "                    for q in range(3): #取q個auxliary item\n",
    "                        aux_np+=a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)\n",
    "                    \"\"\"\n",
    "\n",
    "                    #tf.print('aux attention:',aux_np)\n",
    "                    aux_np+=u #user_latent factor + sum (alpha*auxilary)\n",
    "                    aux_new=tf.assign(aux_new,aux_np) #把aux_new 的 值變成aux_np\n",
    "\n",
    "\n",
    "                    #矩陣中對應函數各自相乘\n",
    "                    # ex: tf.matmul(thetav,(tf.matmul(embedding, image_i, transpose_b=True)))\n",
    "                    xui = tf.matmul(aux_new, vi, transpose_b=True)\n",
    "                    xuj = tf.matmul(aux_new, vj, transpose_b=True)\n",
    "\n",
    "                    xuij = tf.subtract(xui,xuj)\n",
    "\n",
    "                    l2_norm = tf.add_n([\n",
    "                                0.0001 * tf.reduce_sum(tf.multiply(u, u)),\n",
    "                                0.0001 * tf.reduce_sum(tf.multiply(vi, vi)),\n",
    "                                0.0001 * tf.reduce_sum(tf.multiply(vj, vj)),\n",
    "\n",
    "\n",
    "                                0.01 * tf.reduce_sum(tf.multiply(wu, wu)),\n",
    "                                pary_weight * tf.reduce_sum(tf.multiply(wy, wy)),\n",
    "                                pary_weight * tf.reduce_sum(tf.multiply(wa, wa)),\n",
    "                                pary_weight * tf.reduce_sum(tf.multiply(wv,wv)),\n",
    "                                Embedding_weight * tf.reduce_sum(tf.multiply(embedding,embedding)),\n",
    "                                ])\n",
    "\n",
    "                    loss = l2_norm -tf.log(tf.sigmoid(xuij)) # objective funtion\n",
    "                    train_op = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(loss) #parameter optimize \n",
    "                    auc = tf.reduce_mean(tf.to_float(xuij > 0))\n",
    "\n",
    "                    Ur, Yr, Ar, Er, Aur, Ayr, Aar, Avr = training(name+try_i+'_Edims200')\n",
    "                    print('Finish :,',try_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cold YouTuber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd  \n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "import tensorflow as tf\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of data is as follow:\n",
      "user following: (74, 20)\n",
      "feature: (20, 2939)\n",
      "user category: (74, 17)\n",
      "YouTuber category: (74, 17)\n",
      "active user id list: (74,)\n"
     ]
    }
   ],
   "source": [
    "user_following = np.load('user_following_74_20.npy')\n",
    "all_3374 = np.load('all_2939D_y20.npy')\n",
    "user_category = np.load('user_category_74.npy')\n",
    "YouTuber_category = np.load('YouTuber_category_0.7_20.npy')\n",
    "active_users = np.load('active_userID_74.npy')\n",
    "print('The shape of data is as follow:')\n",
    "print('user following:',user_following.shape)\n",
    "print('feature:',all_3374.shape)\n",
    "print('user category:',user_category.shape)\n",
    "print('YouTuber category:',user_category.shape)\n",
    "print('active user id list:',active_users.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt_test_amount = 10\n",
    "user_test_amount = 15\n",
    "yt_num = user_following.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test(user_following,feature,user_category,YouTuber_category,active_users):\n",
    "    yt_test_amount = 10\n",
    "    user_test_amount = 15\n",
    "    user_category_norm = np.zeros(user_category.shape)\n",
    "    for i in range(len(user_category)):\n",
    "        user_category_norm[i] = user_category[i]/np.max(user_category[i])\n",
    "    \n",
    "    user_idx = [i for i in range(len(user_following))]\n",
    "    random.seed(5)\n",
    "    train_t = []\n",
    "    train_f = []\n",
    "    test_t = []\n",
    "    test_f = []\n",
    "    #test_idx = np.zeros(user_test_amount)\n",
    "    more_than_one = []\n",
    "    only_one = []\n",
    "    for i in range(len(user_following)):\n",
    "        if np.sum(user_following,axis=1)[i] > 1:\n",
    "            more_than_one.append(i)\n",
    "        elif np.sum(user_following,axis=1)[i] == 1:\n",
    "            only_one.append(i)\n",
    "    test_idx = more_than_one\n",
    "    rest_test_idx = random.sample(only_one,user_test_amount-len(more_than_one))\n",
    "    test_idx = test_idx + rest_test_idx\n",
    "    \n",
    "    #Train \n",
    "    train_t = []\n",
    "    train_f = []\n",
    "    #Test\n",
    "    test_t = []\n",
    "    test_f = []\n",
    "    test_pos = -1 \n",
    "    for i in range(len(user_following)):\n",
    "        if i in more_than_one: #如果i = 那九個>1的\n",
    "            temp_t = [j for j,v in enumerate(user_following[i]) if v==1]\n",
    "            temp_f = [j for j,v in enumerate(user_following[i]) if v==0]\n",
    "            t_for_test = random.sample(temp_t,math.ceil(0.5*len(temp_t)))\n",
    "            f_for_test  = random.sample(temp_f,yt_test_amount-len(t_for_test))\n",
    "            test_t.append(t_for_test)\n",
    "            test_f.append(f_for_test)\n",
    "            #other for training\n",
    "            t_for_train = [item for item in temp_t if not item in t_for_test]\n",
    "            f_for_train = [item for item in temp_f if not item in f_for_test]\n",
    "            train_t.append(t_for_train)\n",
    "            train_f.append(f_for_train)\n",
    "        elif i in rest_test_idx: #only for test\n",
    "            temp_t = [j for j,v in enumerate(user_following[i]) if v==1]\n",
    "            t_for_test = temp_t\n",
    "            temp_f = [j for j,v in enumerate(user_following[i]) if v==0]\n",
    "            f_for_test = random.sample(temp_f,yt_test_amount-len(t_for_test))\n",
    "            test_t.append(t_for_test)\n",
    "            test_f.append(f_for_test)\n",
    "            #other for training\n",
    "            t_for_train = [item for item in temp_t if not item in t_for_test]\n",
    "            f_for_train = [item for item in temp_f if not item in f_for_test]\n",
    "            train_t.append(t_for_train)\n",
    "            train_f.append(f_for_train)\n",
    "        else:\n",
    "            t_for_train = [j for j,v in enumerate(user_following[i]) if v==1]\n",
    "            f_for_train = [j for j,v in enumerate(user_following[i]) if v==0]\n",
    "            train_t.append(t_for_train)\n",
    "            train_f.append(f_for_train)\n",
    "    \n",
    "    print('The length of train_t:',len(train_t))\n",
    "    print('The length of train_f:',len(train_f))\n",
    "    print('The length of test_t:',len(test_t))\n",
    "    print('The length of test_f:',len(test_f))\n",
    "    return train_t,train_f,test_t,test_f,user_category_norm,test_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of train_t: 74\n",
      "The length of train_f: 74\n",
      "The length of test_t: 15\n",
      "The length of test_f: 15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'for i in range(len(test_t)):\\n    print(test_idx[i])\\n    print(user_following[test_idx[i]])\\n    print(train_t[test_idx[i]])\\n    print(test_t[i])'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_t,train_f,test_t,test_f,user_category_norm,test_idx = get_train_test(user_following,all_3374,user_category,YouTuber_category,active_users)\n",
    "\"\"\"for i in range(len(test_t)):\n",
    "    print(test_idx[i])\n",
    "    print(user_following[test_idx[i]])\n",
    "    print(train_t[test_idx[i]])\n",
    "    print(test_t[i])\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training 0.918918918918919\n",
      "testing 1.0666666666666667\n",
      "比例: 0.10666666666666666\n"
     ]
    }
   ],
   "source": [
    "#average num of following for training user\n",
    "total_train = 0\n",
    "for t in train_t:\n",
    "    total_train += len(t)\n",
    "avg = total_train/len(user_following)\n",
    "print('training',avg)\n",
    "#average num of following for testing user\n",
    "total_test = 0\n",
    "for t in test_t:\n",
    "    total_test += len(t)\n",
    "avg = total_test/user_test_amount\n",
    "print('testing',avg)\n",
    "print('比例:',avg/yt_test_amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_auxilary = [i for i in range(yt_num)]\n",
    "n = len(user_following)\n",
    "m = yt_num  \n",
    "k = 64\n",
    "l = all_3374.shape[1]\n",
    "#embedding_dims = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(save_name): \n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    "    loss_acc_list = []\n",
    "    t0=time.time()\n",
    "    \n",
    "    train_yes_id=[] \n",
    "    for q in range(7):\n",
    "        print('Iteraction:',q)\n",
    "        train_auc=0\n",
    "        total_loss=0\n",
    "        xuij_auc=0\n",
    "        length = 0\n",
    "        for z in range(n):\n",
    "            \"\"\"\n",
    "            yes 用來存放選擇到的YouTuber feature (for auxilary)\n",
    "            yesr 用來存放user對該YouTuber的喜好程度(user_category 跟 YouTuber_category的相似性)\n",
    "            r_3 用來存放user 對該YouTuber種類的偏好(取max)\n",
    "            \"\"\"\n",
    "            yes=[]\n",
    "            yesr=[]\n",
    "        \n",
    "            \"\"\"\n",
    "            only choose positive \n",
    "            \"\"\"\n",
    "            #sample=random.sample(train_t[z],len(train_t[z])) #選全部的Positive\n",
    "            \"\"\"\n",
    "            choose all YouTuber \n",
    "            \"\"\"\n",
    "            sample= all_auxilary #選全部的Positive\n",
    "        \n",
    "            #sample=random.sample(train_t[z]+train_f[z],len(train_t[z])+len(train_f[z]))\n",
    "        \n",
    "            #user degree of category favor \n",
    "            r_3=np.zeros(len(sample)) \n",
    "         \n",
    "            for b in range(len(sample)):\n",
    "                yes.append(all_3374[sample[b]])\n",
    "                yesr.append(YouTuber_category[sample[b]]*user_category_norm[z])\n",
    "        \n",
    "            for b in range(len(yesr)):\n",
    "                r_3[b]=max(yesr[b])\n",
    "            #print('r_3:',r_3)\n",
    "        \n",
    "            yes=np.array(yes)\n",
    "        \n",
    "            #取positive \n",
    "            train_t_sample = random.sample(train_t[z],len(train_t[z]))\n",
    "            #print('number of positive feedback', len(train_t_sample))\n",
    "        \n",
    "            #train_f_sample = random.sample(train_f[z],20)\n",
    "            for ta in train_t_sample:\n",
    "                pos = sample.index(ta)\n",
    "                \n",
    "                image_1=np.expand_dims(all_3374[ta],0) #(1,2048)\n",
    "                if len(train_f[z]) > 10:\n",
    "                    train_f_sample = random.sample(train_f[z],10)\n",
    "                else:\n",
    "                    train_f_sample = random.sample(train_f[z],len(train_f[z]))\n",
    "                \n",
    "                for b in train_f_sample:\n",
    "                    image_2=np.expand_dims(all_3374[b],0) #(1,2048)\n",
    "                    _last_be_relu,_norm_par,_a_list,r3,_auc, _loss,_=sess.run([last_be_relu,norm_par,a_list_smooth,a_list_soft,auc,loss,train_op], feed_dict={user: [z],\n",
    "                                        i: [ta], j: [b], xf: yes , l_id:sample, l_id_len:[len(sample)],positive_id:train_t[z],positive_len:[len(train_t[z])],r:r_3,\n",
    "                                        image_i:image_1,image_j:image_2})\n",
    "                    #print(_a_list)\n",
    "                    #print(r3)\n",
    "                    train_auc+=_auc\n",
    "                    total_loss+=_loss\n",
    "                    length += 1\n",
    "        #print('a_list:',_a_list)\n",
    "        #print('a_list_soft:',r3)\n",
    "        print(\"total_loss:-----------------\", total_loss/length)\n",
    "        print(\"train_auc:-------------------\", train_auc/length)\n",
    "        loss_acc_list.append([total_loss/length,train_auc/length,time.time()-t0])\n",
    "        print('time:',time.time()-t0,' sec')\n",
    "    print('Total cost ',time.time()-t0,' sec')   \n",
    "    U, Y, A, E, Au, Ay, Aa, Av,B =sess.run([user_latent, item_latent, aux_item, embedding, Wu, Wy, Wa, Wv,Beta])\n",
    "    np.savez('./Result/Cold_YouTuber/'+save_name+'.npz', \n",
    "                        U=U, Y=Y, A=A, E=E, Wu=Au, Wy=Ay, Wa=Aa, Wv=Av,B=B)\n",
    "    return U, Y, A, E, Au, Ay, Aa, Av,B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished count ['3', '4', '5', '6', '7', '8', '9', '10', '11']\n",
      "Now : 12\n",
      "Iteraction: 0\n",
      "total_loss:----------------- [[2.3177457]]\n",
      "train_auc:------------------- 0.6199701937406855\n",
      "time: 5.186506509780884  sec\n",
      "Iteraction: 1\n",
      "total_loss:----------------- [[0.8729903]]\n",
      "train_auc:------------------- 0.7526080476900149\n",
      "time: 10.108506917953491  sec\n",
      "Iteraction: 2\n",
      "total_loss:----------------- [[0.46373287]]\n",
      "train_auc:------------------- 0.9329359165424739\n",
      "time: 15.033743858337402  sec\n",
      "Iteraction: 3\n",
      "total_loss:----------------- [[0.2863566]]\n",
      "train_auc:------------------- 0.9791356184798807\n",
      "time: 20.03702664375305  sec\n",
      "Iteraction: 4\n",
      "total_loss:----------------- [[0.22215089]]\n",
      "train_auc:------------------- 0.9940387481371088\n",
      "time: 25.022982597351074  sec\n",
      "Iteraction: 5\n",
      "total_loss:----------------- [[0.18745893]]\n",
      "train_auc:------------------- 0.9940387481371088\n",
      "time: 29.978931665420532  sec\n",
      "Iteraction: 6\n",
      "total_loss:----------------- [[0.15806755]]\n",
      "train_auc:------------------- 0.9970193740685543\n",
      "time: 34.93384289741516  sec\n",
      "Total cost  34.93384289741516  sec\n"
     ]
    }
   ],
   "source": [
    "par_weights = [0.01]\n",
    "beta_weights = [0.001]\n",
    "Embedding_weights = [0.01]\n",
    "Embedding_dims = [200]\n",
    "\n",
    "no_feature = 'Cold_YouTuber_y20_'\n",
    "#all_3374 = np.load('../Data/npy/mask_feature/'+no_feature+'.npy')\n",
    "#l = 2939-2048\n",
    "try_count = [str(try_i+3) for try_i in range(10)]\n",
    "\n",
    "testcount = 0\n",
    "finish_list = []\n",
    "for try_i in try_count:\n",
    "    for pary_weight in par_weights:\n",
    "        for beta_weight in beta_weights:\n",
    "            for Embedding_weight in Embedding_weights:\n",
    "                for embedding_dims in Embedding_dims:\n",
    "                    clear_output()\n",
    "                    print('Finished count',finish_list)\n",
    "                    finish_list.append(try_i)\n",
    "                    print('Now :',try_i)\n",
    "                    \"\"\"\n",
    "                    n: the number of users\n",
    "                    m: the number of YouTubers\n",
    "                    k: latent dims\n",
    "                    l: feature dims\n",
    "                    \"\"\"\n",
    "                    tf.reset_default_graph()\n",
    "\n",
    "                    user = tf.placeholder(tf.int32,shape=(1,))\n",
    "                    i = tf.placeholder(tf.int32, shape=(1,))\n",
    "                    j = tf.placeholder(tf.int32, shape=(1,))\n",
    "\n",
    "                    #多少個auxliary \n",
    "                    xf = tf.placeholder(tf.float32, shape=(None,l))\n",
    "                    l_id = tf.placeholder(tf.int32, shape=(None,))\n",
    "                    l_id_len = tf.placeholder(tf.int32,shape=(1,))\n",
    "                    positive_id = tf.placeholder(tf.int32, shape=(None,))\n",
    "                    positive_len = tf.placeholder(tf.int32,shape=(1,))\n",
    "                    r = tf.placeholder(tf.float32,shape=(None,))\n",
    "\n",
    "\n",
    "                    image_i = tf.placeholder(tf.float32, shape=(1,l))\n",
    "                    image_j = tf.placeholder(tf.float32, shape=(1,l))\n",
    "\n",
    "                    with tf.variable_scope(\"item_level\"):\n",
    "                        user_latent = tf.get_variable(\"user_latent\", [n, k],\n",
    "                                                              initializer=tf.random_normal_initializer(0,0.1,seed=3))\n",
    "                        item_latent = tf.get_variable(\"item_latent\", [m, k],\n",
    "                                                              initializer=tf.random_normal_initializer(0,0.1,seed=3)) \n",
    "                        aux_item = tf.get_variable(\"aux_item\", [m, k],\n",
    "                                                              initializer=tf.random_normal_initializer(0,0.1,seed=3))\n",
    "                        Wu = tf.get_variable(\"Wu\", [n,m,k],  \n",
    "                                                              initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Wy = tf.get_variable(\"Wy\", [n,m,k],   \n",
    "                                                             initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Wa = tf.get_variable(\"Wa\", [n,m,k],  \n",
    "                                                             initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Wv = tf.get_variable(\"Wv\", [n,m,embedding_dims],  \n",
    "                                                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        #Wve =  tf.get_variable(\"Wve\", [embedding_dims,l],  \n",
    "                        #                                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "                        #每個user 對於每個YouTuber都有一個權重\n",
    "                        #w1拿掉，wu\n",
    "                        #hyper?\n",
    "\n",
    "                        aux_new = tf.get_variable(\"aux_new\", [1,k], initializer=tf.constant_initializer(0.0))\n",
    "                        ########## Error part, how to get auxisize dynamically\n",
    "                        ####aux_size= tf.get_variable(name='aux_size', initializer=l_id.get_shape().as_list()[-1])\n",
    "\n",
    "                    with tf.variable_scope('feature_level'):\n",
    "                        embedding = tf.get_variable(\"embedding\", [embedding_dims,l],\n",
    "                                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Beta = tf.get_variable(\"beta\", [n,embedding_dims],\n",
    "                                    initializer=tf.random_normal_initializer(0.01,0.001,seed=10))\n",
    "\n",
    "                    #lookup the latent factors by user and id\n",
    "                    u = tf.nn.embedding_lookup(user_latent, user) #(1*k) 第幾個user latent factor\n",
    "                    vi = tf.nn.embedding_lookup(item_latent, i) \n",
    "                    vj = tf.nn.embedding_lookup(item_latent, j)\n",
    "\n",
    "\n",
    "                    wu = tf.squeeze(tf.nn.embedding_lookup(Wu, user)) #(m*k)\n",
    "                    wy = tf.squeeze(tf.nn.embedding_lookup(Wy, user)) #(m*k)\n",
    "                    wa = tf.squeeze(tf.nn.embedding_lookup(Wa, user)) #(m*k)\n",
    "                    wv = tf.squeeze(tf.nn.embedding_lookup(Wv, user)) #(m,l)\n",
    "                    beta = tf.nn.embedding_lookup(Beta, user) #user feature latent factor\n",
    "\n",
    "\n",
    "                    a_list=tf.Variable([])\n",
    "                    q = tf.constant(0)\n",
    "                    def att_cond(q,a_list):\n",
    "                        return tf.less(q,l_id_len[0])\n",
    "                    def att_body(q,a_list):\n",
    "                        xfi = tf.expand_dims(xf[q],0) #(1,l)\n",
    "                        wuui = tf.expand_dims(tf.nn.embedding_lookup(wu,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        wyui = tf.expand_dims(tf.nn.embedding_lookup(wy,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        waui = tf.expand_dims(tf.nn.embedding_lookup(wa,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        wvui = tf.expand_dims(tf.nn.embedding_lookup(wv,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        a_list = tf.concat([a_list,[(tf.nn.relu( tf.matmul(wuui, u, transpose_b=True) +\n",
    "                                tf.matmul(wyui, tf.expand_dims(tf.nn.embedding_lookup(item_latent,l_id[q]),0), transpose_b=True) +\n",
    "                                tf.matmul(waui, tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0), transpose_b=True) +\n",
    "                                tf.matmul(wvui,tf.matmul(embedding,xfi, transpose_b=True)))[0][0])*r[q]]],0)\n",
    "                        q += 1\n",
    "                        return q,  a_list\n",
    "\n",
    "                    _, a_list = tf.while_loop(att_cond,att_body,[q,a_list],shape_invariants=[q.get_shape(),tf.TensorShape([None])])\n",
    "\n",
    "                    # for while for smoothing\n",
    "                    #a_list_soft=tf.nn.softmax(a_list)\n",
    "                    a_list_smooth = tf.add(a_list,0.0000000001)\n",
    "                    a_list_soft = tf.divide(a_list_smooth,tf.reduce_sum(a_list_smooth, 0)) #without softmax\n",
    "\n",
    "                    norm_par = [wu,wy,wa,wv]\n",
    "\n",
    "                    wuui = tf.expand_dims(tf.nn.embedding_lookup(wu,l_id[-1]),0)\n",
    "                    wyui = tf.expand_dims(tf.nn.embedding_lookup(wy,l_id[-1]),0)\n",
    "                    waui = tf.expand_dims(tf.nn.embedding_lookup(wa,l_id[-1]),0)\n",
    "                    wvui = tf.expand_dims(tf.nn.embedding_lookup(wv,l_id[-1]),0)\n",
    "                    wu_be_relu = tf.matmul(wuui, u, transpose_b=True)\n",
    "                    wy_be_relu = tf.matmul(wyui, tf.expand_dims(tf.nn.embedding_lookup(item_latent,l_id[-1]),0), transpose_b=True)\n",
    "                    wa_be_relu = tf.matmul(waui, tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[-1]),0), transpose_b=True)\n",
    "                    wv_be_relu = tf.matmul(wvui, tf.matmul(embedding,tf.expand_dims(xf[-1],0), transpose_b=True))\n",
    "                    last_be_relu = [wu_be_relu,wy_be_relu,wa_be_relu,wv_be_relu]\n",
    "\n",
    "                    aux_np = tf.expand_dims(tf.zeros(k),0) #dimension (1,32)\n",
    "                    q = tf.constant(0)\n",
    "                    def sum_att_cond(q,aux_np):\n",
    "                        return tf.less(q,l_id_len[0])\n",
    "\n",
    "                    def sum_att_body(q,aux_np):\n",
    "                        #aux_np+=a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)\n",
    "                        aux_np = tf.math.add_n([aux_np,a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)]) \n",
    "                        q += 1\n",
    "                        return q, aux_np\n",
    "\n",
    "                    _,aux_np = tf.while_loop(sum_att_cond,sum_att_body,[q,aux_np])\n",
    "\n",
    "                    \"\"\"\n",
    "                    for q in range(3): #取q個auxliary item\n",
    "                        aux_np+=a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)\n",
    "                    \"\"\"\n",
    "\n",
    "                    aux_part = tf.matmul(aux_np, vi, transpose_b=True)\n",
    "                    #tf.print('aux attention:',aux_np)\n",
    "                    aux_np+=u #user_latent factor + sum (alpha*auxilary)\n",
    "                    aux_new=tf.assign(aux_new,aux_np) #把aux_new 的 值變成aux_np\n",
    "\n",
    "\n",
    "                    latent_i_part = tf.matmul(aux_new, vi, transpose_b=True)\n",
    "                    feature_i_part = tf.matmul(beta,(tf.matmul(embedding,image_i, transpose_b=True)))\n",
    "                    latent_j_part = tf.matmul(aux_new, vj, transpose_b=True)\n",
    "                    feature_j_part = tf.matmul(beta,(tf.matmul(embedding,image_j, transpose_b=True)))\n",
    "                    only_aux_i_part = tf.matmul(aux_np, vi, transpose_b=True)\n",
    "                    only_aux_j_part = tf.matmul(aux_np, vj, transpose_b=True)\n",
    "\n",
    "                    #矩陣中對應函數各自相乘\n",
    "                    # ex: tf.matmul(thetav,(tf.matmul(embedding, image_i, transpose_b=True)))\n",
    "                    xui = tf.matmul(aux_new, vi, transpose_b=True)+ tf.matmul(beta,(tf.matmul(embedding,image_i, transpose_b=True)))\n",
    "                    xuj = tf.matmul(aux_new, vj, transpose_b=True)+ tf.matmul(beta,(tf.matmul(embedding,image_j, transpose_b=True)))\n",
    "\n",
    "                    xuij = tf.subtract(xui,xuj)\n",
    "\n",
    "\n",
    "                    l2_norm = tf.add_n([\n",
    "                                0.0001 * tf.reduce_sum(tf.multiply(u, u)),\n",
    "                                0.0001 * tf.reduce_sum(tf.multiply(vi, vi)),\n",
    "                                0.0001 * tf.reduce_sum(tf.multiply(vj, vj)),\n",
    "\n",
    "\n",
    "                                0.01 * tf.reduce_sum(tf.multiply(wu, wu)),\n",
    "                                pary_weight * tf.reduce_sum(tf.multiply(wy, wy)),\n",
    "                                pary_weight * tf.reduce_sum(tf.multiply(wa, wa)),\n",
    "                                pary_weight * tf.reduce_sum(tf.multiply(wv,wv)),\n",
    "\n",
    "                                beta_weight * tf.reduce_sum(tf.multiply(beta,beta)),\n",
    "                                Embedding_weight * tf.reduce_sum(tf.multiply(embedding,embedding)),\n",
    "\n",
    "                                ])\n",
    "\n",
    "                    loss = l2_norm -tf.log(tf.sigmoid(xuij)) # objective funtion\n",
    "                    train_op = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(loss) #parameter optimize \n",
    "                    auc = tf.reduce_mean(tf.to_float(xuij > 0))\n",
    "\n",
    "                    Ur, Yr, Ar, Er, Aur, Ayr, Aar, Avr,Br = training(no_feature+try_i+'_Edims200')\n",
    "                    #print('Finish count:,',try_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VBPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visual feature shape: (20, 2048)\n"
     ]
    }
   ],
   "source": [
    "image_f = np.load('image_2048D_y20.npy')\n",
    "visual_feature = image_f\n",
    "print('visual feature shape:',visual_feature.shape)\n",
    "l = visual_feature.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(save_name): \n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    "    loss_acc_list = []\n",
    "    t0=time.time()\n",
    "    \n",
    "    train_yes_id=[] \n",
    "    for q in range(7):\n",
    "        print('Iteraction:',q)\n",
    "        train_auc=0\n",
    "        total_loss=0\n",
    "        xuij_auc=0\n",
    "        length = 0\n",
    "        for z in range(n):\n",
    "            \"\"\"\n",
    "            yes 用來存放選擇到的YouTuber feature (for auxilary)\n",
    "            yesr 用來存放user對該YouTuber的喜好程度(user_category 跟 YouTuber_category的相似性)\n",
    "            r_3 用來存放user 對該YouTuber種類的偏好(取max)\n",
    "            \"\"\"\n",
    "        \n",
    "            #取positive \n",
    "            train_t_sample = random.sample(train_t[z],len(train_t[z]))\n",
    "            #print('number of positive feedback', len(train_t_sample))\n",
    "        \n",
    "            #train_f_sample = random.sample(train_f[z],20)\n",
    "            for ta in train_t_sample:\n",
    "                #pos = sample.index(ta)\n",
    "                \n",
    "                image_1=np.expand_dims(visual_feature[ta],0) #(1,2048)\n",
    "                if len(train_f[z]) >= 10:\n",
    "                    train_f_sample = random.sample(train_f[z],10)\n",
    "                else:\n",
    "                    train_f_sample = random.sample(train_f[z],len(train_f[z]))\n",
    "                \n",
    "                \n",
    "                for b in train_f_sample:\n",
    "                    image_2=np.expand_dims(visual_feature[b],0) #(1,2048)\n",
    "                    _auc, _loss,_=sess.run([auc,loss,train_op], feed_dict={user: [z],\n",
    "                                        i: [ta], j: [b],image_i:image_1,image_j:image_2})\n",
    "\n",
    "                    train_auc+=_auc\n",
    "                    total_loss+=_loss\n",
    "                    length += 1\n",
    "\n",
    "        print(\"total_loss:-----------------\", total_loss/length)\n",
    "        print(\"train_auc:-------------------\", train_auc/length)\n",
    "        loss_acc_list.append([total_loss/length,train_auc/length,time.time()-t0])\n",
    "        print('time:',time.time()-t0,' sec')\n",
    "    print('Total cost ',time.time()-t0,' sec')   \n",
    "    U, Y, E,B =sess.run([user_latent, item_latent, embedding,Beta])\n",
    "    np.savez('./Result/Cold_YouTuber/'+save_name+'.npz', \n",
    "                        U=U, Y=Y, E=E,B=B)\n",
    "    return U, Y, E, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Dims [200, 200, 200, 200, 200, 200, 200, 200, 200]\n",
      "Now Dims: 200\n",
      "Iteraction: 0\n",
      "total_loss:----------------- [[2.4277287]]\n",
      "train_auc:------------------- 0.5946348733233979\n",
      "time: 1.1704366207122803  sec\n",
      "Iteraction: 1\n",
      "total_loss:----------------- [[0.94339085]]\n",
      "train_auc:------------------- 0.736214605067064\n",
      "time: 2.253161668777466  sec\n",
      "Iteraction: 2\n",
      "total_loss:----------------- [[0.47071204]]\n",
      "train_auc:------------------- 0.9493293591654247\n",
      "time: 3.3339011669158936  sec\n",
      "Iteraction: 3\n",
      "total_loss:----------------- [[0.30126026]]\n",
      "train_auc:------------------- 0.9791356184798807\n",
      "time: 4.4168407917022705  sec\n",
      "Iteraction: 4\n",
      "total_loss:----------------- [[0.2305603]]\n",
      "train_auc:------------------- 0.9880774962742176\n",
      "time: 5.497425079345703  sec\n",
      "Iteraction: 5\n",
      "total_loss:----------------- [[0.19874705]]\n",
      "train_auc:------------------- 0.9880774962742176\n",
      "time: 6.579292058944702  sec\n",
      "Iteraction: 6\n",
      "total_loss:----------------- [[0.17023575]]\n",
      "train_auc:------------------- 0.9940387481371088\n",
      "time: 7.659980297088623  sec\n",
      "Total cost  7.659980297088623  sec\n",
      "Finish:, 9\n"
     ]
    }
   ],
   "source": [
    "par_weights = [0.01]\n",
    "beta_weights = [0.001]\n",
    "Embedding_weights = [0.01]\n",
    "Embedding_dims = [200]\n",
    "\n",
    "name = 'Cold_YouTuber_y57_VBPR'\n",
    "try_count = [str(try_i) for try_i in range(10)]\n",
    "\n",
    "testcount = 0\n",
    "finish_list = []\n",
    "for try_i in try_count:\n",
    "    for pary_weight in par_weights:\n",
    "        for beta_weight in beta_weights:\n",
    "            for Embedding_weight in Embedding_weights:\n",
    "                for embedding_dims in Embedding_dims:\n",
    "                    clear_output()\n",
    "                    print('Finished Dims',finish_list)\n",
    "                    finish_list.append(embedding_dims)\n",
    "                    print('Now Dims:',embedding_dims)\n",
    "                    \"\"\"\n",
    "                    n: the number of users\n",
    "                    m: the number of YouTubers\n",
    "                    k: latent dims\n",
    "                    l: feature dims\n",
    "                    \"\"\"\n",
    "                    tf.reset_default_graph()\n",
    "\n",
    "                    user = tf.placeholder(tf.int32,shape=(1,))\n",
    "                    i = tf.placeholder(tf.int32, shape=(1,))\n",
    "                    j = tf.placeholder(tf.int32, shape=(1,))\n",
    "\n",
    "                    #多少個auxliary \n",
    "\n",
    "                    image_i = tf.placeholder(tf.float32, shape=(1,l))\n",
    "                    image_j = tf.placeholder(tf.float32, shape=(1,l))\n",
    "\n",
    "                    with tf.variable_scope(\"item_level\"):\n",
    "                        user_latent = tf.get_variable(\"user_latent\", [n, k],\n",
    "                                                              initializer=tf.random_normal_initializer(0,0.1,seed=3))\n",
    "                        item_latent = tf.get_variable(\"item_latent\", [m, k],\n",
    "                                                              initializer=tf.random_normal_initializer(0,0.1,seed=3)) \n",
    "                        #aux_item = tf.get_variable(\"aux_item\", [m, k],\n",
    "                        #                                      initializer=tf.random_normal_initializer(0,0.1,seed=3))\n",
    "                       \n",
    "                        #Wve =  tf.get_variable(\"Wve\", [embedding_dims,l],  \n",
    "                        #                                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "                        #每個user 對於每個YouTuber都有一個權重\n",
    "                        #w1拿掉，wu\n",
    "                        #hyper?\n",
    "\n",
    "                        aux_new = tf.get_variable(\"aux_new\", [1,k], initializer=tf.constant_initializer(0.0))\n",
    "                        ########## Error part, how to get auxisize dynamically\n",
    "                        ####aux_size= tf.get_variable(name='aux_size', initializer=l_id.get_shape().as_list()[-1])\n",
    "\n",
    "                    with tf.variable_scope('feature_level'):\n",
    "                        embedding = tf.get_variable(\"embedding\", [embedding_dims,l],\n",
    "                                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Beta = tf.get_variable(\"beta\", [n,embedding_dims],\n",
    "                                    initializer=tf.random_normal_initializer(0.01,0.001,seed=10))\n",
    "\n",
    "                    #lookup the latent factors by user and id\n",
    "                    u = tf.nn.embedding_lookup(user_latent, user) #(1*k) 第幾個user latent factor\n",
    "                    vi = tf.nn.embedding_lookup(item_latent, i) \n",
    "                    vj = tf.nn.embedding_lookup(item_latent, j)\n",
    "\n",
    "                    beta = tf.nn.embedding_lookup(Beta, user) #user feature latent factor\n",
    "\n",
    "                    \n",
    "                    aux_new=tf.assign(aux_new,u) #把aux_new 的 值變成aux_np\n",
    "                    \n",
    "                    \n",
    "                    #矩陣中對應函數各自相乘\n",
    "                    # ex: tf.matmul(thetav,(tf.matmul(embedding, image_i, transpose_b=True)))\n",
    "                    xui = tf.matmul(aux_new, vi, transpose_b=True)+ tf.matmul(beta,(tf.matmul(embedding,image_i, transpose_b=True)))\n",
    "                    xuj = tf.matmul(aux_new, vj, transpose_b=True)+ tf.matmul(beta,(tf.matmul(embedding,image_j, transpose_b=True)))\n",
    "\n",
    "                    xuij = tf.subtract(xui,xuj)\n",
    "\n",
    "\n",
    "                    l2_norm = tf.add_n([\n",
    "                                0.0001 * tf.reduce_sum(tf.multiply(u, u)),\n",
    "                                0.0001 * tf.reduce_sum(tf.multiply(vi, vi)),\n",
    "                                0.0001 * tf.reduce_sum(tf.multiply(vj, vj)),\n",
    "                                beta_weight * tf.reduce_sum(tf.multiply(beta,beta)),\n",
    "                                Embedding_weight * tf.reduce_sum(tf.multiply(embedding,embedding)),\n",
    "\n",
    "                                ])\n",
    "\n",
    "                    loss = l2_norm -tf.log(tf.sigmoid(xuij)) # objective funtion\n",
    "                    train_op = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(loss) #parameter optimize \n",
    "                    auc = tf.reduce_mean(tf.to_float(xuij > 0))\n",
    "\n",
    "                    Ur, Yr,Er,Br = training(name+try_i+'_Edims200')\n",
    "                    print('Finish:,',try_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ACF_NR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_3374 = np.load('image_2048D_y20.npy')\n",
    "all_auxilary = [i for i in range(yt_num)]\n",
    "n = len(user_following)\n",
    "m = yt_num  \n",
    "k = 64\n",
    "l = all_3374.shape[1]\n",
    "#embedding_dims = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(save_name): \n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    "    loss_acc_list = []\n",
    "    t0=time.time()\n",
    "    \n",
    "    train_yes_id=[] \n",
    "    for q in range(7):\n",
    "        print('Iteraction:',q)\n",
    "        train_auc=0\n",
    "        total_loss=0\n",
    "        xuij_auc=0\n",
    "        length = 0\n",
    "        for z in range(n):\n",
    "            \"\"\"\n",
    "            yes 用來存放選擇到的YouTuber feature (for auxilary)\n",
    "            \"\"\"\n",
    "            yes=[]\n",
    "            yesr=[]\n",
    "        \n",
    "            \"\"\"\n",
    "            only choose positive \n",
    "            \"\"\"\n",
    "            #sample=random.sample(train_t[z],len(train_t[z])) #選全部的Positive\n",
    "            \"\"\"\n",
    "            choose all YouTuber \n",
    "            \"\"\"\n",
    "            sample= all_auxilary #選全部的Positive\n",
    "        \n",
    "            #sample=random.sample(train_t[z]+train_f[z],len(train_t[z])+len(train_f[z]))\n",
    "        \n",
    "            \n",
    "         \n",
    "            for b in range(len(sample)):\n",
    "                yes.append(all_3374[sample[b]])\n",
    "        \n",
    "            yes=np.array(yes)\n",
    "        \n",
    "            #取positive \n",
    "            train_t_sample = random.sample(train_t[z],len(train_t[z]))\n",
    "            #print('number of positive feedback', len(train_t_sample))\n",
    "        \n",
    "            #train_f_sample = random.sample(train_f[z],20)\n",
    "            for ta in train_t_sample:\n",
    "                pos = sample.index(ta)\n",
    "                \n",
    "                image_1=np.expand_dims(all_3374[ta],0) #(1,2048)\n",
    "                if len(train_f[z]) >= 10:\n",
    "                    train_f_sample = random.sample(train_f[z],10)\n",
    "                else:\n",
    "                    train_f_sample = random.sample(train_f[z],len(train_f[z]))\n",
    "                \n",
    "                for b in train_f_sample:\n",
    "                    image_2=np.expand_dims(all_3374[b],0) #(1,2048)\n",
    "                    _a_list,r3,_auc, _loss,_=sess.run([a_list_smooth,a_list_soft,auc,loss,train_op], feed_dict={user: [z],\n",
    "                                        i: [ta], j: [b], xf: yes , l_id:sample, l_id_len:[len(sample)],positive_id:train_t[z],positive_len:[len(train_t[z])],\n",
    "                                        image_i:image_1,image_j:image_2})\n",
    "                    #print(_a_list)\n",
    "                    #print(r3)\n",
    "                    train_auc+=_auc\n",
    "                    total_loss+=_loss\n",
    "                    length += 1\n",
    "        #print('a_list:',_a_list)\n",
    "        #print('a_list_soft:',r3)\n",
    "        print(\"total_loss:-----------------\", total_loss/length)\n",
    "        print(\"train_auc:-------------------\", train_auc/length)\n",
    "        loss_acc_list.append([total_loss/length,train_auc/length,time.time()-t0])\n",
    "        print('time:',time.time()-t0,' sec')\n",
    "    print('Total cost ',time.time()-t0,' sec')   \n",
    "    U, Y, A,E,Au, Ay, Aa, Av =sess.run([user_latent, item_latent, aux_item, embedding,Wu, Wy, Wa, Wv])\n",
    "    np.savez('./Result/Cold_YouTuber/'+save_name+'.npz', \n",
    "                        U=U, Y=Y, A=A,E=E,Wu=Au, Wy=Ay, Wa=Aa, Wv=Av)\n",
    "    return U, Y, A, E, Au, Ay, Aa, Av"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Img0\n",
      "Iteraction: 0\n",
      "total_loss:----------------- [[1.5539416]]\n",
      "train_auc:------------------- 0.5230998509687034\n",
      "time: 4.332263946533203  sec\n",
      "Iteraction: 1\n",
      "total_loss:----------------- [[0.7000997]]\n",
      "train_auc:------------------- 0.5603576751117735\n",
      "time: 8.521077871322632  sec\n",
      "Iteraction: 2\n",
      "total_loss:----------------- [[0.6816559]]\n",
      "train_auc:------------------- 0.6080476900149031\n",
      "time: 12.695041179656982  sec\n",
      "Iteraction: 3\n",
      "total_loss:----------------- [[0.6707354]]\n",
      "train_auc:------------------- 0.6810730253353204\n",
      "time: 16.83790612220764  sec\n",
      "Iteraction: 4\n",
      "total_loss:----------------- [[0.6572317]]\n",
      "train_auc:------------------- 0.736214605067064\n",
      "time: 21.02732253074646  sec\n",
      "Iteraction: 5\n",
      "total_loss:----------------- [[0.65351754]]\n",
      "train_auc:------------------- 0.7496274217585693\n",
      "time: 25.23172903060913  sec\n",
      "Iteraction: 6\n",
      "total_loss:----------------- [[0.6514266]]\n",
      "train_auc:------------------- 0.7853949329359166\n",
      "time: 29.42029047012329  sec\n",
      "Total cost  29.42029047012329  sec\n",
      "Finish : Img0\n",
      "Img1\n",
      "Iteraction: 0\n",
      "total_loss:----------------- [[1.5495558]]\n",
      "train_auc:------------------- 0.5216095380029806\n",
      "time: 4.360936164855957  sec\n",
      "Iteraction: 1\n",
      "total_loss:----------------- [[0.69599336]]\n",
      "train_auc:------------------- 0.6020864381520119\n",
      "time: 8.580644130706787  sec\n",
      "Iteraction: 2\n",
      "total_loss:----------------- [[0.6803081]]\n",
      "train_auc:------------------- 0.6274217585692996\n",
      "time: 12.818490028381348  sec\n",
      "Iteraction: 3\n",
      "total_loss:----------------- [[0.6704945]]\n",
      "train_auc:------------------- 0.6825633383010432\n",
      "time: 17.02379035949707  sec\n",
      "Iteraction: 4\n",
      "total_loss:----------------- [[0.66535115]]\n",
      "train_auc:------------------- 0.7049180327868853\n",
      "time: 21.260889053344727  sec\n",
      "Iteraction: 5\n",
      "total_loss:----------------- [[0.6592813]]\n",
      "train_auc:------------------- 0.7511177347242921\n",
      "time: 25.51302480697632  sec\n",
      "Iteraction: 6\n",
      "total_loss:----------------- [[0.6581692]]\n",
      "train_auc:------------------- 0.7481371087928465\n",
      "time: 29.733872175216675  sec\n",
      "Total cost  29.733872175216675  sec\n",
      "Finish : Img1\n",
      "Img2\n",
      "Iteraction: 0\n",
      "total_loss:----------------- [[1.5647696]]\n",
      "train_auc:------------------- 0.496274217585693\n",
      "time: 4.3770506381988525  sec\n",
      "Iteraction: 1\n",
      "total_loss:----------------- [[0.6992131]]\n",
      "train_auc:------------------- 0.5842026825633383\n",
      "time: 8.612888097763062  sec\n",
      "Iteraction: 2\n",
      "total_loss:----------------- [[0.6816177]]\n",
      "train_auc:------------------- 0.6318926974664679\n",
      "time: 12.865679025650024  sec\n",
      "Iteraction: 3\n",
      "total_loss:----------------- [[0.67170006]]\n",
      "train_auc:------------------- 0.6736214605067065\n",
      "time: 17.07094168663025  sec\n",
      "Iteraction: 4\n",
      "total_loss:----------------- [[0.6640866]]\n",
      "train_auc:------------------- 0.7108792846497765\n",
      "time: 21.339680194854736  sec\n",
      "Iteraction: 5\n",
      "total_loss:----------------- [[0.65608907]]\n",
      "train_auc:------------------- 0.7645305514157973\n",
      "time: 25.57704734802246  sec\n",
      "Iteraction: 6\n",
      "total_loss:----------------- [[0.6569679]]\n",
      "train_auc:------------------- 0.7704918032786885\n",
      "time: 29.812594652175903  sec\n",
      "Total cost  29.812594652175903  sec\n",
      "Finish : Img2\n",
      "Img3\n",
      "Iteraction: 0\n",
      "total_loss:----------------- [[1.5630913]]\n",
      "train_auc:------------------- 0.4903129657228018\n",
      "time: 4.548647403717041  sec\n",
      "Iteraction: 1\n",
      "total_loss:----------------- [[0.7076954]]\n",
      "train_auc:------------------- 0.5022354694485842\n",
      "time: 8.931042194366455  sec\n",
      "Iteraction: 2\n",
      "total_loss:----------------- [[0.6895248]]\n",
      "train_auc:------------------- 0.5916542473919523\n",
      "time: 13.312385320663452  sec\n",
      "Iteraction: 3\n",
      "total_loss:----------------- [[0.6805404]]\n",
      "train_auc:------------------- 0.6378539493293591\n",
      "time: 17.690569162368774  sec\n",
      "Iteraction: 4\n",
      "total_loss:----------------- [[0.67157483]]\n",
      "train_auc:------------------- 0.6900149031296572\n",
      "time: 22.066707611083984  sec\n",
      "Iteraction: 5\n",
      "total_loss:----------------- [[0.6667701]]\n",
      "train_auc:------------------- 0.7570789865871833\n",
      "time: 26.459394693374634  sec\n",
      "Iteraction: 6\n",
      "total_loss:----------------- [[0.65676254]]\n",
      "train_auc:------------------- 0.7704918032786885\n",
      "time: 30.805349588394165  sec\n",
      "Total cost  30.805349588394165  sec\n",
      "Finish : Img3\n",
      "Img4\n",
      "Iteraction: 0\n",
      "total_loss:----------------- [[1.5592401]]\n",
      "train_auc:------------------- 0.4843517138599106\n",
      "time: 4.266752481460571  sec\n",
      "Iteraction: 1\n",
      "total_loss:----------------- [[0.7027537]]\n",
      "train_auc:------------------- 0.5514157973174366\n",
      "time: 8.440748929977417  sec\n",
      "Iteraction: 2\n",
      "total_loss:----------------- [[0.6836383]]\n",
      "train_auc:------------------- 0.5976154992548435\n",
      "time: 12.603158473968506  sec\n",
      "Iteraction: 3\n",
      "total_loss:----------------- [[0.67580867]]\n",
      "train_auc:------------------- 0.6363636363636364\n",
      "time: 16.751104593276978  sec\n",
      "Iteraction: 4\n",
      "total_loss:----------------- [[0.6654943]]\n",
      "train_auc:------------------- 0.6959761549925484\n",
      "time: 21.00704574584961  sec\n",
      "Iteraction: 5\n",
      "total_loss:----------------- [[0.6612064]]\n",
      "train_auc:------------------- 0.7153502235469449\n",
      "time: 25.318722009658813  sec\n",
      "Iteraction: 6\n",
      "total_loss:----------------- [[0.65508294]]\n",
      "train_auc:------------------- 0.7734724292101341\n",
      "time: 29.49577760696411  sec\n",
      "Total cost  29.49577760696411  sec\n",
      "Finish : Img4\n",
      "Img5\n",
      "Iteraction: 0\n",
      "total_loss:----------------- [[1.5490096]]\n",
      "train_auc:------------------- 0.5320417287630402\n",
      "time: 4.608701705932617  sec\n",
      "Iteraction: 1\n",
      "total_loss:----------------- [[0.699634]]\n",
      "train_auc:------------------- 0.5767511177347243\n",
      "time: 9.030259370803833  sec\n",
      "Iteraction: 2\n",
      "total_loss:----------------- [[0.6825759]]\n",
      "train_auc:------------------- 0.639344262295082\n",
      "time: 13.558135986328125  sec\n",
      "Iteraction: 3\n",
      "total_loss:----------------- [[0.67380464]]\n",
      "train_auc:------------------- 0.669150521609538\n",
      "time: 18.035707473754883  sec\n",
      "Iteraction: 4\n",
      "total_loss:----------------- [[0.6652996]]\n",
      "train_auc:------------------- 0.7198211624441133\n",
      "time: 22.49514675140381  sec\n",
      "Iteraction: 5\n",
      "total_loss:----------------- [[0.656494]]\n",
      "train_auc:------------------- 0.7660208643815202\n",
      "time: 26.912823915481567  sec\n",
      "Iteraction: 6\n",
      "total_loss:----------------- [[0.65882206]]\n",
      "train_auc:------------------- 0.7615499254843517\n",
      "time: 31.281136989593506  sec\n",
      "Total cost  31.281136989593506  sec\n",
      "Finish : Img5\n",
      "Img6\n",
      "Iteraction: 0\n",
      "total_loss:----------------- [[1.5584195]]\n",
      "train_auc:------------------- 0.481371087928465\n",
      "time: 4.532256364822388  sec\n",
      "Iteraction: 1\n",
      "total_loss:----------------- [[0.70198196]]\n",
      "train_auc:------------------- 0.5469448584202683\n",
      "time: 8.955556869506836  sec\n",
      "Iteraction: 2\n",
      "total_loss:----------------- [[0.6833134]]\n",
      "train_auc:------------------- 0.6035767511177347\n",
      "time: 13.393408298492432  sec\n",
      "Iteraction: 3\n",
      "total_loss:----------------- [[0.67247015]]\n",
      "train_auc:------------------- 0.6944858420268256\n",
      "time: 17.801010131835938  sec\n",
      "Iteraction: 4\n",
      "total_loss:----------------- [[0.66271037]]\n",
      "train_auc:------------------- 0.7302533532041728\n",
      "time: 22.239845037460327  sec\n",
      "Iteraction: 5\n",
      "total_loss:----------------- [[0.65657157]]\n",
      "train_auc:------------------- 0.7868852459016393\n",
      "time: 26.66318416595459  sec\n",
      "Iteraction: 6\n",
      "total_loss:----------------- [[0.6563148]]\n",
      "train_auc:------------------- 0.7704918032786885\n",
      "time: 31.101987600326538  sec\n",
      "Total cost  31.101987600326538  sec\n",
      "Finish : Img6\n",
      "Img7\n",
      "Iteraction: 0\n",
      "total_loss:----------------- [[1.5580345]]\n",
      "train_auc:------------------- 0.4918032786885246\n",
      "time: 4.6163411140441895  sec\n",
      "Iteraction: 1\n",
      "total_loss:----------------- [[0.7035743]]\n",
      "train_auc:------------------- 0.5230998509687034\n",
      "time: 9.06123948097229  sec\n",
      "Iteraction: 2\n",
      "total_loss:----------------- [[0.6879495]]\n",
      "train_auc:------------------- 0.5752608047690015\n",
      "time: 13.515370607376099  sec\n",
      "Iteraction: 3\n",
      "total_loss:----------------- [[0.6746053]]\n",
      "train_auc:------------------- 0.6438152011922503\n",
      "time: 18.0008282661438  sec\n",
      "Iteraction: 4\n",
      "total_loss:----------------- [[0.66873413]]\n",
      "train_auc:------------------- 0.6929955290611028\n",
      "time: 22.45612120628357  sec\n",
      "Iteraction: 5\n",
      "total_loss:----------------- [[0.66533005]]\n",
      "train_auc:------------------- 0.7049180327868853\n",
      "time: 26.912200927734375  sec\n",
      "Iteraction: 6\n",
      "total_loss:----------------- [[0.6554707]]\n",
      "train_auc:------------------- 0.7630402384500745\n",
      "time: 31.35260820388794  sec\n",
      "Total cost  31.35260820388794  sec\n",
      "Finish : Img7\n",
      "Img8\n",
      "Iteraction: 0\n",
      "total_loss:----------------- [[1.5630049]]\n",
      "train_auc:------------------- 0.47242921013412814\n",
      "time: 4.579375505447388  sec\n",
      "Iteraction: 1\n",
      "total_loss:----------------- [[0.7008917]]\n",
      "train_auc:------------------- 0.5543964232488823\n",
      "time: 9.033825159072876  sec\n",
      "Iteraction: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_loss:----------------- [[0.6839026]]\n",
      "train_auc:------------------- 0.6005961251862891\n",
      "time: 13.488155841827393  sec\n",
      "Iteraction: 3\n",
      "total_loss:----------------- [[0.67134804]]\n",
      "train_auc:------------------- 0.6572280178837556\n",
      "time: 17.943376064300537  sec\n",
      "Iteraction: 4\n",
      "total_loss:----------------- [[0.66122454]]\n",
      "train_auc:------------------- 0.713859910581222\n",
      "time: 22.3983314037323  sec\n",
      "Iteraction: 5\n",
      "total_loss:----------------- [[0.6581761]]\n",
      "train_auc:------------------- 0.7391952309985097\n",
      "time: 26.86928081512451  sec\n",
      "Iteraction: 6\n",
      "total_loss:----------------- [[0.65210915]]\n",
      "train_auc:------------------- 0.7779433681073026\n",
      "time: 31.292625904083252  sec\n",
      "Total cost  31.292625904083252  sec\n",
      "Finish : Img8\n",
      "Img9\n",
      "Iteraction: 0\n",
      "total_loss:----------------- [[1.5578066]]\n",
      "train_auc:------------------- 0.496274217585693\n",
      "time: 4.426656723022461  sec\n",
      "Iteraction: 1\n",
      "total_loss:----------------- [[0.70426553]]\n",
      "train_auc:------------------- 0.5514157973174366\n",
      "time: 8.693340063095093  sec\n",
      "Iteraction: 2\n",
      "total_loss:----------------- [[0.6860825]]\n",
      "train_auc:------------------- 0.6020864381520119\n",
      "time: 12.961048126220703  sec\n",
      "Iteraction: 3\n",
      "total_loss:----------------- [[0.67826223]]\n",
      "train_auc:------------------- 0.646795827123696\n",
      "time: 17.228842735290527  sec\n",
      "Iteraction: 4\n",
      "total_loss:----------------- [[0.6688573]]\n",
      "train_auc:------------------- 0.706408345752608\n",
      "time: 21.496500730514526  sec\n",
      "Iteraction: 5\n",
      "total_loss:----------------- [[0.6638256]]\n",
      "train_auc:------------------- 0.7257824143070045\n",
      "time: 25.732826948165894  sec\n",
      "Iteraction: 6\n",
      "total_loss:----------------- [[0.6554973]]\n",
      "train_auc:------------------- 0.7615499254843517\n",
      "time: 29.968537092208862  sec\n",
      "Total cost  29.968537092208862  sec\n",
      "Finish : Img9\n"
     ]
    }
   ],
   "source": [
    "par_weights = [0.01]\n",
    "beta_weights = [0.001]\n",
    "Embedding_weights = [0.01]\n",
    "Embedding_dims = [200]\n",
    "\n",
    "name = 'Cold_YouTuber_y57_ACF_NR'\n",
    "count_try = ['Img'+str(i) for i in range(10)]\n",
    "\n",
    "testcount = 0\n",
    "finish_list = []\n",
    "for try_i in count_try:\n",
    "    for pary_weight in par_weights:\n",
    "        for beta_weight in beta_weights:\n",
    "            for Embedding_weight in Embedding_weights:\n",
    "                for embedding_dims in Embedding_dims:\n",
    "                    #clear_output()\n",
    "                    #print('Finished Dims',finish_list)\n",
    "                    finish_list.append(embedding_dims)\n",
    "                    #print('Now Dims:',embedding_dims)\n",
    "                    print(try_i)\n",
    "                    \"\"\"\n",
    "                    n: the number of users\n",
    "                    m: the number of YouTubers\n",
    "                    k: latent dims\n",
    "                    l: feature dims\n",
    "                    \"\"\"\n",
    "                    tf.reset_default_graph()\n",
    "\n",
    "                    user = tf.placeholder(tf.int32,shape=(1,))\n",
    "                    i = tf.placeholder(tf.int32, shape=(1,))\n",
    "                    j = tf.placeholder(tf.int32, shape=(1,))\n",
    "\n",
    "                    #多少個auxliary \n",
    "                    xf = tf.placeholder(tf.float32, shape=(None,l))\n",
    "                    l_id = tf.placeholder(tf.int32, shape=(None,))\n",
    "                    l_id_len = tf.placeholder(tf.int32,shape=(1,))\n",
    "                    positive_id = tf.placeholder(tf.int32, shape=(None,))\n",
    "                    positive_len = tf.placeholder(tf.int32,shape=(1,))\n",
    "\n",
    "\n",
    "                    image_i = tf.placeholder(tf.float32, shape=(1,l))\n",
    "                    image_j = tf.placeholder(tf.float32, shape=(1,l))\n",
    "\n",
    "                    with tf.variable_scope(\"item_level\"):\n",
    "                        user_latent = tf.get_variable(\"user_latent\", [n, k],\n",
    "                                                              initializer=tf.random_normal_initializer(0,0.1,seed=3))\n",
    "                        item_latent = tf.get_variable(\"item_latent\", [m, k],\n",
    "                                                              initializer=tf.random_normal_initializer(0,0.1,seed=3)) \n",
    "                        aux_item = tf.get_variable(\"aux_item\", [m, k],\n",
    "                                                              initializer=tf.random_normal_initializer(0,0.1,seed=3))\n",
    "                        Wu = tf.get_variable(\"Wu\", [n,m,k],  \n",
    "                                                              initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Wy = tf.get_variable(\"Wy\", [n,m,k],   \n",
    "                                                             initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Wa = tf.get_variable(\"Wa\", [n,m,k],  \n",
    "                                                             initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Wv = tf.get_variable(\"Wv\", [n,m,embedding_dims],  \n",
    "                                                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        #Wve =  tf.get_variable(\"Wve\", [embedding_dims,l],  \n",
    "                        #                                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "                        with tf.variable_scope('feature_level'):\n",
    "                            embedding = tf.get_variable(\"embedding\", [embedding_dims,l],\n",
    "                                        initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "                        aux_new = tf.get_variable(\"aux_new\", [1,k], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "\n",
    "\n",
    "                    #lookup the latent factors by user and id\n",
    "                    u = tf.nn.embedding_lookup(user_latent, user) #(1*k) 第幾個user latent factor\n",
    "                    vi = tf.nn.embedding_lookup(item_latent, i) \n",
    "                    vj = tf.nn.embedding_lookup(item_latent, j)\n",
    "\n",
    "\n",
    "                    wu = tf.squeeze(tf.nn.embedding_lookup(Wu, user)) #(m*k)\n",
    "                    wy = tf.squeeze(tf.nn.embedding_lookup(Wy, user)) #(m*k)\n",
    "                    wa = tf.squeeze(tf.nn.embedding_lookup(Wa, user)) #(m*k)\n",
    "                    wv = tf.squeeze(tf.nn.embedding_lookup(Wv, user)) #(m,l)\n",
    "\n",
    "\n",
    "                    a_list=tf.Variable([])\n",
    "                    q = tf.constant(0)\n",
    "                    def att_cond(q,a_list):\n",
    "                        return tf.less(q,l_id_len[0])\n",
    "                    def att_body(q,a_list):\n",
    "                        xfi = tf.expand_dims(xf[q],0) #(1,l)\n",
    "                        wuui = tf.expand_dims(tf.nn.embedding_lookup(wu,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        wyui = tf.expand_dims(tf.nn.embedding_lookup(wy,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        waui = tf.expand_dims(tf.nn.embedding_lookup(wa,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        wvui = tf.expand_dims(tf.nn.embedding_lookup(wv,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        a_list = tf.concat([a_list,[(tf.nn.relu( tf.matmul(wuui, u, transpose_b=True) +\n",
    "                                tf.matmul(wyui, tf.expand_dims(tf.nn.embedding_lookup(item_latent,l_id[q]),0), transpose_b=True) +\n",
    "                                tf.matmul(waui, tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0), transpose_b=True) +\n",
    "                                tf.matmul(wvui,tf.matmul(embedding,xfi, transpose_b=True)))[0][0])]],0)\n",
    "                        q += 1\n",
    "                        return q,  a_list\n",
    "\n",
    "                    _, a_list = tf.while_loop(att_cond,att_body,[q,a_list],shape_invariants=[q.get_shape(),tf.TensorShape([None])])\n",
    "\n",
    "                    # for while for smoothing\n",
    "                    #a_list_soft=tf.nn.softmax(a_list)\n",
    "                    a_list_smooth = tf.add(a_list,0.0000000001)\n",
    "                    a_list_soft = tf.divide(a_list_smooth,tf.reduce_sum(a_list_smooth, 0)) #without softmax\n",
    "\n",
    "                    aux_np = tf.expand_dims(tf.zeros(k),0) #dimension (1,32)\n",
    "                    q = tf.constant(0)\n",
    "                    def sum_att_cond(q,aux_np):\n",
    "                        return tf.less(q,l_id_len[0])\n",
    "\n",
    "                    def sum_att_body(q,aux_np):\n",
    "                        #aux_np+=a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)\n",
    "                        aux_np = tf.math.add_n([aux_np,a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)]) \n",
    "                        q += 1\n",
    "                        return q, aux_np\n",
    "\n",
    "                    _,aux_np = tf.while_loop(sum_att_cond,sum_att_body,[q,aux_np])\n",
    "\n",
    "                    \"\"\"\n",
    "                    for q in range(3): #取q個auxliary item\n",
    "                        aux_np+=a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)\n",
    "                    \"\"\"\n",
    "\n",
    "                    #tf.print('aux attention:',aux_np)\n",
    "                    aux_np+=u #user_latent factor + sum (alpha*auxilary)\n",
    "                    aux_new=tf.assign(aux_new,aux_np) #把aux_new 的 值變成aux_np\n",
    "\n",
    "\n",
    "                    #矩陣中對應函數各自相乘\n",
    "                    # ex: tf.matmul(thetav,(tf.matmul(embedding, image_i, transpose_b=True)))\n",
    "                    xui = tf.matmul(aux_new, vi, transpose_b=True)\n",
    "                    xuj = tf.matmul(aux_new, vj, transpose_b=True)\n",
    "\n",
    "                    xuij = tf.subtract(xui,xuj)\n",
    "\n",
    "                    l2_norm = tf.add_n([\n",
    "                                0.0001 * tf.reduce_sum(tf.multiply(u, u)),\n",
    "                                0.0001 * tf.reduce_sum(tf.multiply(vi, vi)),\n",
    "                                0.0001 * tf.reduce_sum(tf.multiply(vj, vj)),\n",
    "\n",
    "\n",
    "                                0.01 * tf.reduce_sum(tf.multiply(wu, wu)),\n",
    "                                pary_weight * tf.reduce_sum(tf.multiply(wy, wy)),\n",
    "                                pary_weight * tf.reduce_sum(tf.multiply(wa, wa)),\n",
    "                                pary_weight * tf.reduce_sum(tf.multiply(wv,wv)),\n",
    "                                Embedding_weight * tf.reduce_sum(tf.multiply(embedding,embedding)),\n",
    "                                ])\n",
    "\n",
    "                    loss = l2_norm -tf.log(tf.sigmoid(xuij)) # objective funtion\n",
    "                    train_op = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(loss) #parameter optimize \n",
    "                    auc = tf.reduce_mean(tf.to_float(xuij > 0))\n",
    "\n",
    "                    Ur, Yr, Ar, Er, Aur, Ayr, Aar, Avr = training(name+try_i+'_Edims200')\n",
    "                    print('Finish :', try_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ACF_R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(save_name): \n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    "    loss_acc_list = []\n",
    "    t0=time.time()\n",
    "    \n",
    "    train_yes_id=[] \n",
    "    for q in range(7):\n",
    "        print('Iteraction:',q)\n",
    "        train_auc=0\n",
    "        total_loss=0\n",
    "        xuij_auc=0\n",
    "        length = 0\n",
    "        for z in range(n):\n",
    "            \"\"\"\n",
    "            yes 用來存放選擇到的YouTuber feature (for auxilary)\n",
    "            yesr 用來存放user對該YouTuber的喜好程度(user_category 跟 YouTuber_category的相似性)\n",
    "            r_3 用來存放user 對該YouTuber種類的偏好(取max)\n",
    "            \"\"\"\n",
    "            yes=[]\n",
    "            yesr=[]\n",
    "        \n",
    "            \"\"\"\n",
    "            only choose positive \n",
    "            \"\"\"\n",
    "            #sample=random.sample(train_t[z],len(train_t[z])) #選全部的Positive\n",
    "            \"\"\"\n",
    "            choose all YouTuber \n",
    "            \"\"\"\n",
    "            sample= all_auxilary #選全部的Positive\n",
    "        \n",
    "            #sample=random.sample(train_t[z]+train_f[z],len(train_t[z])+len(train_f[z]))\n",
    "        \n",
    "            #user degree of category favor \n",
    "            r_3=np.zeros(len(sample)) \n",
    "         \n",
    "            for b in range(len(sample)):\n",
    "                yes.append(all_3374[sample[b]])\n",
    "                yesr.append(YouTuber_category[sample[b]]*user_category_norm[z])\n",
    "        \n",
    "            for b in range(len(yesr)):\n",
    "                r_3[b]=max(yesr[b])\n",
    "            #print('r_3:',r_3)\n",
    "        \n",
    "            yes=np.array(yes)\n",
    "        \n",
    "            #取positive \n",
    "            train_t_sample = random.sample(train_t[z],len(train_t[z]))\n",
    "            #print('number of positive feedback', len(train_t_sample))\n",
    "        \n",
    "            #train_f_sample = random.sample(train_f[z],20)\n",
    "            for ta in train_t_sample:\n",
    "                pos = sample.index(ta)\n",
    "                \n",
    "                image_1=np.expand_dims(all_3374[ta],0) #(1,2048)\n",
    "                if len(train_f[z]) >= 10:\n",
    "                    train_f_sample = random.sample(train_f[z],10)\n",
    "                else:\n",
    "                    train_f_sample = random.sample(train_f[z],len(train_f[z]))\n",
    "                \n",
    "                for b in train_f_sample:\n",
    "                    image_2=np.expand_dims(all_3374[b],0) #(1,2048)\n",
    "                    _a_list,r3,_auc, _loss,_=sess.run([a_list_smooth,a_list_soft,auc,loss,train_op], feed_dict={user: [z],\n",
    "                                        i: [ta], j: [b], xf: yes , l_id:sample, l_id_len:[len(sample)],positive_id:train_t[z],positive_len:[len(train_t[z])],r:r_3,\n",
    "                                        image_i:image_1,image_j:image_2})\n",
    "                    #print(_a_list)\n",
    "                    #print(r3)\n",
    "                    train_auc+=_auc\n",
    "                    total_loss+=_loss\n",
    "                    length += 1\n",
    "        #print('a_list:',_a_list)\n",
    "        #print('a_list_soft:',r3)\n",
    "        print(\"total_loss:-----------------\", total_loss/length)\n",
    "        print(\"train_auc:-------------------\", train_auc/length)\n",
    "        loss_acc_list.append([total_loss/length,train_auc/length,time.time()-t0])\n",
    "        print('time:',time.time()-t0,' sec')\n",
    "    print('Total cost ',time.time()-t0,' sec')   \n",
    "    U, Y, A,E,Au, Ay, Aa, Av =sess.run([user_latent, item_latent, aux_item, embedding,Wu, Wy, Wa, Wv])\n",
    "    np.savez('./Result/Cold_YouTuber/'+save_name+'.npz'+save_name+'.npz', \n",
    "                        U=U, Y=Y, A=A,E=E,Wu=Au, Wy=Ay, Wa=Aa, Wv=Av)\n",
    "    return U, Y, A, E, Au, Ay, Aa, Av"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Img0\n",
      "Iteraction: 0\n",
      "total_loss:----------------- [[1.5424972]]\n",
      "train_auc:------------------- 0.5797317436661699\n",
      "time: 4.696911573410034  sec\n",
      "Iteraction: 1\n",
      "total_loss:----------------- [[0.69139004]]\n",
      "train_auc:------------------- 0.6140089418777943\n",
      "time: 9.24565315246582  sec\n",
      "Iteraction: 2\n",
      "total_loss:----------------- [[0.6647303]]\n",
      "train_auc:------------------- 0.7019374068554396\n",
      "time: 13.841882944107056  sec\n",
      "Iteraction: 3\n",
      "total_loss:----------------- [[0.654725]]\n",
      "train_auc:------------------- 0.7287630402384501\n",
      "time: 18.423915147781372  sec\n",
      "Iteraction: 4\n",
      "total_loss:----------------- [[0.6432023]]\n",
      "train_auc:------------------- 0.7615499254843517\n",
      "time: 23.004926204681396  sec\n",
      "Iteraction: 5\n",
      "total_loss:----------------- [[0.6315474]]\n",
      "train_auc:------------------- 0.7988077496274217\n",
      "time: 27.55485224723816  sec\n",
      "Iteraction: 6\n",
      "total_loss:----------------- [[0.6286138]]\n",
      "train_auc:------------------- 0.7958271236959762\n",
      "time: 32.14904713630676  sec\n",
      "Total cost  32.14904713630676  sec\n",
      "Finish :, Img0\n",
      "Img1\n",
      "Iteraction: 0\n",
      "total_loss:----------------- [[1.5474104]]\n",
      "train_auc:------------------- 0.5543964232488823\n",
      "time: 4.517733335494995  sec\n",
      "Iteraction: 1\n",
      "total_loss:----------------- [[0.6841333]]\n",
      "train_auc:------------------- 0.624441132637854\n",
      "time: 8.864616394042969  sec\n",
      "Iteraction: 2\n",
      "total_loss:----------------- [[0.66655964]]\n",
      "train_auc:------------------- 0.6706408345752608\n",
      "time: 13.242051362991333  sec\n",
      "Iteraction: 3\n",
      "total_loss:----------------- [[0.65634596]]\n",
      "train_auc:------------------- 0.7257824143070045\n",
      "time: 17.619975805282593  sec\n",
      "Iteraction: 4\n",
      "total_loss:----------------- [[0.6429488]]\n",
      "train_auc:------------------- 0.7421758569299552\n",
      "time: 21.996533632278442  sec\n",
      "Iteraction: 5\n",
      "total_loss:----------------- [[0.6341318]]\n",
      "train_auc:------------------- 0.7868852459016393\n",
      "time: 26.3735249042511  sec\n",
      "Iteraction: 6\n",
      "total_loss:----------------- [[0.6246943]]\n",
      "train_auc:------------------- 0.797317436661699\n",
      "time: 30.7658634185791  sec\n",
      "Total cost  30.7658634185791  sec\n",
      "Finish :, Img1\n",
      "Img2\n",
      "Iteraction: 0\n",
      "total_loss:----------------- [[1.5222619]]\n",
      "train_auc:------------------- 0.6080476900149031\n",
      "time: 4.707064628601074  sec\n",
      "Iteraction: 1\n",
      "total_loss:----------------- [[0.6741036]]\n",
      "train_auc:------------------- 0.6944858420268256\n",
      "time: 9.272460222244263  sec\n",
      "Iteraction: 2\n",
      "total_loss:----------------- [[0.65952116]]\n",
      "train_auc:------------------- 0.7183308494783904\n",
      "time: 13.83665680885315  sec\n",
      "Iteraction: 3\n",
      "total_loss:----------------- [[0.6509017]]\n",
      "train_auc:------------------- 0.7466467958271237\n",
      "time: 18.4163715839386  sec\n",
      "Iteraction: 4\n",
      "total_loss:----------------- [[0.6352265]]\n",
      "train_auc:------------------- 0.7883755588673621\n",
      "time: 22.982810258865356  sec\n",
      "Iteraction: 5\n",
      "total_loss:----------------- [[0.63379025]]\n",
      "train_auc:------------------- 0.7809239940387481\n",
      "time: 27.50022864341736  sec\n",
      "Iteraction: 6\n",
      "total_loss:----------------- [[0.6262222]]\n",
      "train_auc:------------------- 0.8152011922503726\n",
      "time: 32.08366584777832  sec\n",
      "Total cost  32.08366584777832  sec\n",
      "Finish :, Img2\n",
      "Img3\n",
      "Iteraction: 0\n",
      "total_loss:----------------- [[1.54164]]\n",
      "train_auc:------------------- 0.5812220566318927\n",
      "time: 4.642488956451416  sec\n",
      "Iteraction: 1\n",
      "total_loss:----------------- [[0.68192303]]\n",
      "train_auc:------------------- 0.6631892697466468\n",
      "time: 9.206828355789185  sec\n",
      "Iteraction: 2\n",
      "total_loss:----------------- [[0.66563857]]\n",
      "train_auc:------------------- 0.7004470938897168\n",
      "time: 13.740018606185913  sec\n",
      "Iteraction: 3\n",
      "total_loss:----------------- [[0.65639085]]\n",
      "train_auc:------------------- 0.7436661698956781\n",
      "time: 18.273738384246826  sec\n",
      "Iteraction: 4\n",
      "total_loss:----------------- [[0.6471031]]\n",
      "train_auc:------------------- 0.7585692995529061\n",
      "time: 22.789809942245483  sec\n",
      "Iteraction: 5\n",
      "total_loss:----------------- [[0.64039797]]\n",
      "train_auc:------------------- 0.7958271236959762\n",
      "time: 27.33954620361328  sec\n",
      "Iteraction: 6\n",
      "total_loss:----------------- [[0.632797]]\n",
      "train_auc:------------------- 0.7883755588673621\n",
      "time: 31.857229232788086  sec\n",
      "Total cost  31.857229232788086  sec\n",
      "Finish :, Img3\n",
      "Img4\n",
      "Iteraction: 0\n",
      "total_loss:----------------- [[1.5348744]]\n",
      "train_auc:------------------- 0.6408345752608048\n",
      "time: 4.500256299972534  sec\n",
      "Iteraction: 1\n",
      "total_loss:----------------- [[0.6805709]]\n",
      "train_auc:------------------- 0.6810730253353204\n",
      "time: 8.90852165222168  sec\n",
      "Iteraction: 2\n",
      "total_loss:----------------- [[0.65936935]]\n",
      "train_auc:------------------- 0.7540983606557377\n",
      "time: 13.331804513931274  sec\n",
      "Iteraction: 3\n",
      "total_loss:----------------- [[0.65141565]]\n",
      "train_auc:------------------- 0.7585692995529061\n",
      "time: 17.740201234817505  sec\n",
      "Iteraction: 4\n",
      "total_loss:----------------- [[0.6346593]]\n",
      "train_auc:------------------- 0.8152011922503726\n",
      "time: 22.157799005508423  sec\n",
      "Iteraction: 5\n",
      "total_loss:----------------- [[0.6392474]]\n",
      "train_auc:------------------- 0.8077496274217586\n",
      "time: 26.58225989341736  sec\n",
      "Iteraction: 6\n",
      "total_loss:----------------- [[0.6242058]]\n",
      "train_auc:------------------- 0.819672131147541\n",
      "time: 30.99039363861084  sec\n",
      "Total cost  30.99039363861084  sec\n",
      "Finish :, Img4\n",
      "Img5\n",
      "Iteraction: 0\n",
      "total_loss:----------------- [[1.5424615]]\n",
      "train_auc:------------------- 0.5901639344262295\n",
      "time: 4.656962633132935  sec\n",
      "Iteraction: 1\n",
      "total_loss:----------------- [[0.6847435]]\n",
      "train_auc:------------------- 0.6050670640834576\n",
      "time: 9.12686824798584  sec\n",
      "Iteraction: 2\n",
      "total_loss:----------------- [[0.65892464]]\n",
      "train_auc:------------------- 0.7019374068554396\n",
      "time: 13.612584590911865  sec\n",
      "Iteraction: 3\n",
      "total_loss:----------------- [[0.6521374]]\n",
      "train_auc:------------------- 0.7257824143070045\n",
      "time: 18.098074436187744  sec\n",
      "Iteraction: 4\n",
      "total_loss:----------------- [[0.6400618]]\n",
      "train_auc:------------------- 0.767511177347243\n",
      "time: 22.584677696228027  sec\n",
      "Iteraction: 5\n",
      "total_loss:----------------- [[0.64053863]]\n",
      "train_auc:------------------- 0.7824143070044709\n",
      "time: 27.03888988494873  sec\n",
      "Iteraction: 6\n",
      "total_loss:----------------- [[0.6229674]]\n",
      "train_auc:------------------- 0.8152011922503726\n",
      "time: 31.509883165359497  sec\n",
      "Total cost  31.509883165359497  sec\n",
      "Finish :, Img5\n",
      "Img6\n",
      "Iteraction: 0\n",
      "total_loss:----------------- [[1.5411644]]\n",
      "train_auc:------------------- 0.5707898658718331\n",
      "time: 4.612010478973389  sec\n",
      "Iteraction: 1\n",
      "total_loss:----------------- [[0.67611986]]\n",
      "train_auc:------------------- 0.6408345752608048\n",
      "time: 9.12802267074585  sec\n",
      "Iteraction: 2\n",
      "total_loss:----------------- [[0.6632418]]\n",
      "train_auc:------------------- 0.6661698956780924\n",
      "time: 13.629291534423828  sec\n",
      "Iteraction: 3\n",
      "total_loss:----------------- [[0.6481483]]\n",
      "train_auc:------------------- 0.7421758569299552\n",
      "time: 18.11471915245056  sec\n",
      "Iteraction: 4\n",
      "total_loss:----------------- [[0.6468393]]\n",
      "train_auc:------------------- 0.7600596125186289\n",
      "time: 22.618399381637573  sec\n",
      "Iteraction: 5\n",
      "total_loss:----------------- [[0.6302528]]\n",
      "train_auc:------------------- 0.8062593144560357\n",
      "time: 27.120079040527344  sec\n",
      "Iteraction: 6\n",
      "total_loss:----------------- [[0.63451266]]\n",
      "train_auc:------------------- 0.797317436661699\n",
      "time: 31.621310472488403  sec\n",
      "Total cost  31.621310472488403  sec\n",
      "Finish :, Img6\n",
      "Img7\n",
      "Iteraction: 0\n",
      "total_loss:----------------- [[1.5386153]]\n",
      "train_auc:------------------- 0.5782414307004471\n",
      "time: 4.516066312789917  sec\n",
      "Iteraction: 1\n",
      "total_loss:----------------- [[0.67908233]]\n",
      "train_auc:------------------- 0.6289120715350224\n",
      "time: 8.876410245895386  sec\n",
      "Iteraction: 2\n",
      "total_loss:----------------- [[0.6651925]]\n",
      "train_auc:------------------- 0.6587183308494784\n",
      "time: 13.252292156219482  sec\n",
      "Iteraction: 3\n",
      "total_loss:----------------- [[0.6545408]]\n",
      "train_auc:------------------- 0.6870342771982116\n",
      "time: 17.675142765045166  sec\n",
      "Iteraction: 4\n",
      "total_loss:----------------- [[0.64512]]\n",
      "train_auc:------------------- 0.7391952309985097\n",
      "time: 22.036041736602783  sec\n",
      "Iteraction: 5\n",
      "total_loss:----------------- [[0.64470136]]\n",
      "train_auc:------------------- 0.7436661698956781\n",
      "time: 26.36514687538147  sec\n",
      "Iteraction: 6\n",
      "total_loss:----------------- [[0.63667524]]\n",
      "train_auc:------------------- 0.7555886736214605\n",
      "time: 30.756632328033447  sec\n",
      "Total cost  30.756632328033447  sec\n",
      "Finish :, Img7\n",
      "Img8\n",
      "Iteraction: 0\n",
      "total_loss:----------------- [[1.5298789]]\n",
      "train_auc:------------------- 0.61698956780924\n",
      "time: 4.6567065715789795  sec\n",
      "Iteraction: 1\n",
      "total_loss:----------------- [[0.6784429]]\n",
      "train_auc:------------------- 0.6646795827123696\n",
      "time: 9.204393148422241  sec\n",
      "Iteraction: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_loss:----------------- [[0.6540197]]\n",
      "train_auc:------------------- 0.6944858420268256\n",
      "time: 13.787182331085205  sec\n",
      "Iteraction: 3\n",
      "total_loss:----------------- [[0.64426136]]\n",
      "train_auc:------------------- 0.7198211624441133\n",
      "time: 18.357734441757202  sec\n",
      "Iteraction: 4\n",
      "total_loss:----------------- [[0.62589395]]\n",
      "train_auc:------------------- 0.7988077496274217\n",
      "time: 22.87491250038147  sec\n",
      "Iteraction: 5\n",
      "total_loss:----------------- [[0.62694335]]\n",
      "train_auc:------------------- 0.8062593144560357\n",
      "time: 27.423691511154175  sec\n",
      "Iteraction: 6\n",
      "total_loss:----------------- [[0.6151811]]\n",
      "train_auc:------------------- 0.8152011922503726\n",
      "time: 31.97222137451172  sec\n",
      "Total cost  31.97222137451172  sec\n",
      "Finish :, Img8\n",
      "Img9\n",
      "Iteraction: 0\n",
      "total_loss:----------------- [[1.5317556]]\n",
      "train_auc:------------------- 0.61698956780924\n",
      "time: 4.501065492630005  sec\n",
      "Iteraction: 1\n",
      "total_loss:----------------- [[0.6680244]]\n",
      "train_auc:------------------- 0.6497764530551415\n",
      "time: 8.862563848495483  sec\n",
      "Iteraction: 2\n",
      "total_loss:----------------- [[0.6471386]]\n",
      "train_auc:------------------- 0.7198211624441133\n",
      "time: 13.238742113113403  sec\n",
      "Iteraction: 3\n",
      "total_loss:----------------- [[0.639571]]\n",
      "train_auc:------------------- 0.7555886736214605\n",
      "time: 17.59918522834778  sec\n",
      "Iteraction: 4\n",
      "total_loss:----------------- [[0.635452]]\n",
      "train_auc:------------------- 0.7958271236959762\n",
      "time: 21.960942268371582  sec\n",
      "Iteraction: 5\n",
      "total_loss:----------------- [[0.6233853]]\n",
      "train_auc:------------------- 0.8181818181818182\n",
      "time: 26.369011878967285  sec\n",
      "Iteraction: 6\n",
      "total_loss:----------------- [[0.61936784]]\n",
      "train_auc:------------------- 0.8092399403874814\n",
      "time: 30.74510359764099  sec\n",
      "Total cost  30.74510359764099  sec\n",
      "Finish :, Img9\n"
     ]
    }
   ],
   "source": [
    "par_weights = [0.01]\n",
    "beta_weights = [0.001]\n",
    "Embedding_weights = [0.01]\n",
    "Embedding_dims = [200]\n",
    "\n",
    "name = 'Cold_YouTuber_y57_ACF_R'\n",
    "count_try = ['Img'+str(i) for i in range(10)]\n",
    "\n",
    "testcount = 0\n",
    "finish_list = []\n",
    "for try_i in count_try:\n",
    "    for pary_weight in par_weights:\n",
    "        for beta_weight in beta_weights:\n",
    "            for Embedding_weight in Embedding_weights:\n",
    "                for embedding_dims in Embedding_dims:\n",
    "                    #clear_output()\n",
    "                    #print('Finished Dims',finish_list)\n",
    "                    finish_list.append(embedding_dims)\n",
    "                    #print('Now Dims:',embedding_dims)\n",
    "                    print(try_i)\n",
    "                    \"\"\"\n",
    "                    n: the number of users\n",
    "                    m: the number of YouTubers\n",
    "                    k: latent dims\n",
    "                    l: feature dims\n",
    "                    \"\"\"\n",
    "                    tf.reset_default_graph()\n",
    "\n",
    "                    user = tf.placeholder(tf.int32,shape=(1,))\n",
    "                    i = tf.placeholder(tf.int32, shape=(1,))\n",
    "                    j = tf.placeholder(tf.int32, shape=(1,))\n",
    "\n",
    "                    #多少個auxliary \n",
    "                    xf = tf.placeholder(tf.float32, shape=(None,l))\n",
    "                    l_id = tf.placeholder(tf.int32, shape=(None,))\n",
    "                    l_id_len = tf.placeholder(tf.int32,shape=(1,))\n",
    "                    positive_id = tf.placeholder(tf.int32, shape=(None,))\n",
    "                    positive_len = tf.placeholder(tf.int32,shape=(1,))\n",
    "                    r = tf.placeholder(tf.float32,shape=(None,))\n",
    "\n",
    "\n",
    "                    image_i = tf.placeholder(tf.float32, shape=(1,l))\n",
    "                    image_j = tf.placeholder(tf.float32, shape=(1,l))\n",
    "\n",
    "                    with tf.variable_scope(\"item_level\"):\n",
    "                        user_latent = tf.get_variable(\"user_latent\", [n, k],\n",
    "                                                              initializer=tf.random_normal_initializer(0,0.1,seed=3))\n",
    "                        item_latent = tf.get_variable(\"item_latent\", [m, k],\n",
    "                                                              initializer=tf.random_normal_initializer(0,0.1,seed=3)) \n",
    "                        aux_item = tf.get_variable(\"aux_item\", [m, k],\n",
    "                                                              initializer=tf.random_normal_initializer(0,0.1,seed=3))\n",
    "                        Wu = tf.get_variable(\"Wu\", [n,m,k],  \n",
    "                                                              initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Wy = tf.get_variable(\"Wy\", [n,m,k],   \n",
    "                                                             initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Wa = tf.get_variable(\"Wa\", [n,m,k],  \n",
    "                                                             initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        Wv = tf.get_variable(\"Wv\", [n,m,embedding_dims],  \n",
    "                                                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        #Wve =  tf.get_variable(\"Wve\", [embedding_dims,l],  \n",
    "                        #                                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "                        with tf.variable_scope('feature_level'):\n",
    "                            embedding = tf.get_variable(\"embedding\", [embedding_dims,l],\n",
    "                                        initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "                        aux_new = tf.get_variable(\"aux_new\", [1,k], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "\n",
    "\n",
    "                    #lookup the latent factors by user and id\n",
    "                    u = tf.nn.embedding_lookup(user_latent, user) #(1*k) 第幾個user latent factor\n",
    "                    vi = tf.nn.embedding_lookup(item_latent, i) \n",
    "                    vj = tf.nn.embedding_lookup(item_latent, j)\n",
    "\n",
    "\n",
    "                    wu = tf.squeeze(tf.nn.embedding_lookup(Wu, user)) #(m*k)\n",
    "                    wy = tf.squeeze(tf.nn.embedding_lookup(Wy, user)) #(m*k)\n",
    "                    wa = tf.squeeze(tf.nn.embedding_lookup(Wa, user)) #(m*k)\n",
    "                    wv = tf.squeeze(tf.nn.embedding_lookup(Wv, user)) #(m,l)\n",
    "\n",
    "\n",
    "                    a_list=tf.Variable([])\n",
    "                    q = tf.constant(0)\n",
    "                    def att_cond(q,a_list):\n",
    "                        return tf.less(q,l_id_len[0])\n",
    "                    def att_body(q,a_list):\n",
    "                        xfi = tf.expand_dims(xf[q],0) #(1,l)\n",
    "                        wuui = tf.expand_dims(tf.nn.embedding_lookup(wu,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        wyui = tf.expand_dims(tf.nn.embedding_lookup(wy,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        waui = tf.expand_dims(tf.nn.embedding_lookup(wa,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        wvui = tf.expand_dims(tf.nn.embedding_lookup(wv,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "                        a_list = tf.concat([a_list,[(tf.nn.relu( tf.matmul(wuui, u, transpose_b=True) +\n",
    "                                tf.matmul(wyui, tf.expand_dims(tf.nn.embedding_lookup(item_latent,l_id[q]),0), transpose_b=True) +\n",
    "                                tf.matmul(waui, tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0), transpose_b=True) +\n",
    "                                tf.matmul(wvui,tf.matmul(embedding,xfi, transpose_b=True)))[0][0])*r[q]]],0)\n",
    "                        q += 1\n",
    "                        return q,  a_list\n",
    "\n",
    "                    _, a_list = tf.while_loop(att_cond,att_body,[q,a_list],shape_invariants=[q.get_shape(),tf.TensorShape([None])])\n",
    "\n",
    "                    # for while for smoothing\n",
    "                    #a_list_soft=tf.nn.softmax(a_list)\n",
    "                    a_list_smooth = tf.add(a_list,0.0000000001)\n",
    "                    a_list_soft = tf.divide(a_list_smooth,tf.reduce_sum(a_list_smooth, 0)) #without softmax\n",
    "\n",
    "                    aux_np = tf.expand_dims(tf.zeros(k),0) #dimension (1,32)\n",
    "                    q = tf.constant(0)\n",
    "                    def sum_att_cond(q,aux_np):\n",
    "                        return tf.less(q,l_id_len[0])\n",
    "\n",
    "                    def sum_att_body(q,aux_np):\n",
    "                        #aux_np+=a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)\n",
    "                        aux_np = tf.math.add_n([aux_np,a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)]) \n",
    "                        q += 1\n",
    "                        return q, aux_np\n",
    "\n",
    "                    _,aux_np = tf.while_loop(sum_att_cond,sum_att_body,[q,aux_np])\n",
    "\n",
    "                    \"\"\"\n",
    "                    for q in range(3): #取q個auxliary item\n",
    "                        aux_np+=a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)\n",
    "                    \"\"\"\n",
    "\n",
    "                    #tf.print('aux attention:',aux_np)\n",
    "                    aux_np+=u #user_latent factor + sum (alpha*auxilary)\n",
    "                    aux_new=tf.assign(aux_new,aux_np) #把aux_new 的 值變成aux_np\n",
    "\n",
    "\n",
    "                    #矩陣中對應函數各自相乘\n",
    "                    # ex: tf.matmul(thetav,(tf.matmul(embedding, image_i, transpose_b=True)))\n",
    "                    xui = tf.matmul(aux_new, vi, transpose_b=True)\n",
    "                    xuj = tf.matmul(aux_new, vj, transpose_b=True)\n",
    "\n",
    "                    xuij = tf.subtract(xui,xuj)\n",
    "\n",
    "                    l2_norm = tf.add_n([\n",
    "                                0.0001 * tf.reduce_sum(tf.multiply(u, u)),\n",
    "                                0.0001 * tf.reduce_sum(tf.multiply(vi, vi)),\n",
    "                                0.0001 * tf.reduce_sum(tf.multiply(vj, vj)),\n",
    "\n",
    "\n",
    "                                0.01 * tf.reduce_sum(tf.multiply(wu, wu)),\n",
    "                                pary_weight * tf.reduce_sum(tf.multiply(wy, wy)),\n",
    "                                pary_weight * tf.reduce_sum(tf.multiply(wa, wa)),\n",
    "                                pary_weight * tf.reduce_sum(tf.multiply(wv,wv)),\n",
    "                                Embedding_weight * tf.reduce_sum(tf.multiply(embedding,embedding)),\n",
    "                                ])\n",
    "\n",
    "                    loss = l2_norm -tf.log(tf.sigmoid(xuij)) # objective funtion\n",
    "                    train_op = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(loss) #parameter optimize \n",
    "                    auc = tf.reduce_mean(tf.to_float(xuij > 0))\n",
    "\n",
    "                    Ur, Yr, Ar, Er, Aur, Ayr, Aar, Avr = training(name+try_i+'_Edims200')\n",
    "                    print('Finish :,',try_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
